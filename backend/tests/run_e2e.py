"""
å…¨è‡ªåŠ¨ç«¯åˆ°ç«¯å¤šè½®å¯¹è¯æµ‹è¯•è¿è¡Œå™¨

æ ¸å¿ƒç‰¹æ€§ï¼š
1. è‡ªåŠ¨é™æµé‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼Œæœ€å¤š5æ¬¡ï¼Œä»15ç§’å¼€å§‹ï¼‰
2. åœºæ™¯é—´è‡ªåŠ¨æ’å…¥å†·å´é—´éš”
3. è¿›åº¦æŒä¹…åŒ–åˆ° JSONï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘
4. å…¨ç¨‹æ— äººå€¼å®ˆï¼Œæœ€ç»ˆæ±‡æ€»è¯„ä¼°æŠ¥å‘Š

API é™æµ: 10/min â†’ æ¯åˆ†é’Ÿæœ€å¤š10æ¬¡è°ƒç”¨
æ¯ä¸ªåœºæ™¯: 4-5è½® Ã— 2æ¬¡è°ƒç”¨(å¯¹è¯+è¯„ä¼°) = 8-10æ¬¡
ç­–ç•¥: æ¯æ¬¡ API è°ƒç”¨é—´éš” 7sï¼Œæ¯ä¸ªåœºæ™¯å®Œæˆåé¢å¤–ç­‰ 15s
"""
import asyncio
import sys
import os
import json
import time
import traceback

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from dotenv import load_dotenv
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), ".env"))

from test_e2e_multi_turn import (
    E2EConversationEngine, LLMJudge, 
    ALL_SCENARIOS, TestScenario, TestTurn,
    ScenarioResult, TurnResult,
)
from llm.provider import LLMProviderFactory, LLMMessage, LLMConfig

PROGRESS_FILE = "/tmp/e2e_progress.json"
RESULT_FILE = "/tmp/e2e_final_report.json"

# ============================================================
# å¸¦é™æµé‡è¯•çš„ API è°ƒç”¨å°è£…
# ============================================================

async def call_with_retry(coro_func, max_retries=5, base_delay=15):
    """å¸¦æŒ‡æ•°é€€é¿é‡è¯•çš„ API è°ƒç”¨"""
    for attempt in range(max_retries + 1):
        try:
            result = await coro_func()
            return result
        except Exception as e:
            err_str = str(e)
            if "429" in err_str or "rate" in err_str.lower() or "é™æµ" in err_str:
                if attempt < max_retries:
                    delay = base_delay * (2 ** attempt)  # 15, 30, 60, 120, 240
                    delay = min(delay, 120)  # æœ€å¤šç­‰120ç§’
                    print(f"      â³ é™æµï¼Œç­‰å¾… {delay}s åé‡è¯• ({attempt+1}/{max_retries})...")
                    await asyncio.sleep(delay)
                    continue
            raise  # éé™æµé”™è¯¯ç›´æ¥æŠ›å‡º


# ============================================================
# å¸¦é™æµä¿æŠ¤çš„åœºæ™¯è¿è¡Œå™¨
# ============================================================

async def run_scenario_safe(scenario: TestScenario, call_interval: float = 7.0) -> ScenarioResult:
    """å¸¦é™æµä¿æŠ¤çš„åœºæ™¯è¿è¡Œ"""
    result = ScenarioResult(name=scenario.name, description=scenario.description)
    
    engine = E2EConversationEngine()
    judge = LLMJudge()
    
    conversation_so_far = []
    
    for i, turn in enumerate(scenario.turns):
        print(f"    è½®æ¬¡ {i+1}/{len(scenario.turns)}: {turn.query[:50]}...")
        
        # --- å¯¹è¯è°ƒç”¨ï¼ˆå¸¦é‡è¯•ï¼‰---
        start_time = time.time()
        try:
            response = await call_with_retry(lambda t=turn: engine.chat(t.query))
        except Exception as e:
            result.errors.append(f"è½®æ¬¡{i+1} LLM è°ƒç”¨å¤±è´¥(é‡è¯•åä»å¤±è´¥): {str(e)}")
            print(f"      âŒ LLM è°ƒç”¨æœ€ç»ˆå¤±è´¥: {e}")
            conversation_so_far.append({"role": "user", "content": turn.query})
            conversation_so_far.append({"role": "assistant", "content": "[è°ƒç”¨å¤±è´¥]"})
            continue
        latency = time.time() - start_time
        
        print(f"      âœ“ å›å¤ ({latency:.1f}s): {response[:80]}...")
        
        # å†·å´é—´éš”
        await asyncio.sleep(call_interval)
        
        # --- è¯„ä¼°è°ƒç”¨ï¼ˆå¸¦é‡è¯•ï¼‰---
        try:
            eval_result = await call_with_retry(
                lambda conv=list(conversation_so_far), q=turn.query, r=response, ef=turn.evaluation_focus: 
                    judge.evaluate_turn(conv, q, r, ef)
            )
        except Exception as e:
            print(f"      âš ï¸ è¯„ä¼°æœ€ç»ˆå¤±è´¥: {e}")
            eval_result = {
                "scores": {"context_utilization": 5, "reference_resolution": 5, 
                          "information_accuracy": 5, "coherence": 5, "helpfulness": 5},
                "reasoning": f"è¯„ä¼°è°ƒç”¨å¤±è´¥: {str(e)}",
                "improvement_suggestions": ""
            }
        
        scores = eval_result.get("scores", {})
        avg_score = sum(scores.values()) / len(scores) if scores else 0
        
        turn_result = TurnResult(
            turn_index=i + 1,
            user_query=turn.query,
            assistant_response=response,
            latency_seconds=latency,
            scores=scores,
            evaluation_reasoning=eval_result.get("reasoning", ""),
        )
        result.turns.append(turn_result)
        
        score_str = " | ".join(f"{k}:{v}" for k, v in scores.items())
        print(f"      ğŸ“Š è¯„åˆ† [avg={avg_score:.1f}]: {score_str}")
        
        if avg_score < turn.min_expected_score:
            msg = f"è½®æ¬¡{i+1} å¹³å‡åˆ† {avg_score:.1f} ä½äºé¢„æœŸ {turn.min_expected_score}"
            result.errors.append(msg)
            print(f"      âš ï¸ {msg}")
            suggestions = eval_result.get("improvement_suggestions", "")
            if suggestions:
                print(f"      ğŸ’¡ å»ºè®®: {suggestions[:200]}")
        
        conversation_so_far.append({"role": "user", "content": turn.query})
        conversation_so_far.append({"role": "assistant", "content": response})
        
        # æ¯è½®å¯¹è¯ä¹‹é—´å†·å´
        if i < len(scenario.turns) - 1:
            await asyncio.sleep(call_interval)
    
    # è®¡ç®—æ€»åˆ†
    all_scores = []
    for t in result.turns:
        if t.scores:
            all_scores.extend(t.scores.values())
    result.overall_score = sum(all_scores) / len(all_scores) if all_scores else 0
    
    return result


def save_progress(all_results, total_scenarios, phase="running"):
    """ä¿å­˜ä¸­é—´è¿›åº¦"""
    data = {
        "phase": phase,
        "completed": len(all_results),
        "total": total_scenarios,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "results": []
    }
    for r in all_results:
        data["results"].append({
            "name": r.name,
            "score": round(r.overall_score, 2),
            "passed": r.passed,
            "errors": r.errors,
            "turns": [{
                "turn": t.turn_index,
                "scores": t.scores,
                "avg_score": round(sum(t.scores.values()) / len(t.scores), 2) if t.scores else 0,
                "reasoning": t.evaluation_reasoning[:500],
                "query": t.user_query,
                "response": t.assistant_response[:300],
                "latency": round(t.latency_seconds, 1),
            } for t in r.turns]
        })
    
    with open(PROGRESS_FILE, "w") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def generate_final_report(all_results):
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    print()
    print("=" * 70)
    print("ğŸ“Š ç«¯åˆ°ç«¯å¤šè½®å¯¹è¯æµ‹è¯• - æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š")
    print("=" * 70)
    print()
    
    # ç»´åº¦æ±‡æ€»
    dim_scores = {}
    for r in all_results:
        for t in r.turns:
            for d, s in t.scores.items():
                dim_scores.setdefault(d, []).append(s)
    
    print("ğŸ“ˆ å„ç»´åº¦å¹³å‡åˆ†:")
    low_dims = []
    for d in ["context_utilization", "reference_resolution", "information_accuracy", "coherence", "helpfulness"]:
        scores = dim_scores.get(d, [])
        if not scores:
            continue
        avg = sum(scores) / len(scores)
        bar = "â–ˆ" * int(avg) + "â–‘" * (10 - int(avg))
        flag = "âœ…" if avg >= 7 else ("âš ï¸" if avg >= 5 else "âŒ")
        print(f"  {flag} {d:30s} {bar} {avg:.1f}/10 (n={len(scores)})")
        if avg < 7:
            low_dims.append((d, avg))
    
    print()
    print("ğŸ“‹ å„åœºæ™¯æ€»åˆ†:")
    passed = 0
    for r in all_results:
        flag = "âœ…" if r.passed else "âŒ"
        print(f"  {flag} {r.name:30s} {r.overall_score:.1f}/10", end="")
        if r.errors:
            print(f"  ({len(r.errors)} ä¸ªé—®é¢˜)")
        else:
            print()
        if r.passed:
            passed += 1
    
    total = len(all_results)
    print(f"\nğŸ æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    # ä½åˆ†åˆ†æ
    if low_dims:
        print()
        print("=" * 70)
        print("âš ï¸ ä½åˆ†ç»´åº¦åˆ†æä¸ä¼˜åŒ–å»ºè®®")
        print("=" * 70)
        for dim, avg in low_dims:
            print(f"\n  ğŸ“‰ [{dim}] å¹³å‡åˆ†: {avg:.1f}")
            for r in all_results:
                for t in r.turns:
                    if dim in t.scores and t.scores[dim] < 7:
                        print(f"    - åœºæ™¯ã€Œ{r.name}ã€è½®æ¬¡{t.turn_index}: {dim}={t.scores[dim]}")
                        print(f"      é—®: {t.user_query[:60]}")
                        print(f"      ç­”: {t.assistant_response[:100]}...")
                        if t.evaluation_reasoning:
                            print(f"      è¯„è¯­: {t.evaluation_reasoning[:200]}")
    
    # è¯¦ç»†è¯„åˆ†
    print()
    print("=" * 70)
    print("ğŸ“ è¯¦ç»†è¯„åˆ†æ•°æ®")
    print("=" * 70)
    for r in all_results:
        print(f"\n  åœºæ™¯: {r.name} (æ€»åˆ†: {r.overall_score:.1f})")
        for t in r.turns:
            avg = sum(t.scores.values()) / len(t.scores) if t.scores else 0
            print(f"    Turn {t.turn_index} [avg={avg:.1f}]: {', '.join(f'{k}={v}' for k,v in t.scores.items())}")
            print(f"      Q: {t.user_query[:70]}")
            print(f"      A: {t.assistant_response[:120]}...")
    
    # ä¿å­˜æœ€ç»ˆ JSON æŠ¥å‘Š
    save_progress(all_results, total, phase="completed")
    
    report = {
        "summary": {
            "total_scenarios": total,
            "passed": passed,
            "failed": total - passed,
            "dimension_averages": {d: round(sum(s)/len(s), 2) for d, s in dim_scores.items()},
            "low_dimensions": [(d, round(a, 2)) for d, a in low_dims],
        },
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
    }
    with open(RESULT_FILE, "w") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {RESULT_FILE}")
    
    return low_dims


async def main():
    start_time = time.time()
    all_results = []
    
    print("=" * 70)
    print("ğŸš€ ç«¯åˆ°ç«¯å¤šè½®å¯¹è¯æµ‹è¯• - å…¨è‡ªåŠ¨è¿è¡Œ")
    print(f"   API é™æµ: 10/min, è°ƒç”¨é—´éš”: 7s, åœºæ™¯å†·å´: 20s")
    print(f"   å…± {len(ALL_SCENARIOS)} ä¸ªåœºæ™¯, é¢„è®¡è€—æ—¶ ~{len(ALL_SCENARIOS) * 3}åˆ†é’Ÿ")
    print("=" * 70)
    
    for idx, scenario in enumerate(ALL_SCENARIOS):
        print(f"\n{'='*60}")
        print(f"  [{idx+1}/{len(ALL_SCENARIOS)}] åœºæ™¯: {scenario.name}")
        print(f"  æè¿°: {scenario.description}")
        print(f"{'='*60}")
        
        result = await run_scenario_safe(scenario, call_interval=7.0)
        all_results.append(result)
        
        status = "âœ… PASS" if result.passed else "âŒ FAIL"
        print(f"\n  {status} {result.name} (æ€»åˆ†: {result.overall_score:.1f}/10)")
        if result.errors:
            for e in result.errors:
                print(f"    âš ï¸ {e}")
        
        # ä¿å­˜è¿›åº¦
        save_progress(all_results, len(ALL_SCENARIOS))
        
        # åœºæ™¯é—´å†·å´ï¼ˆæœ€åä¸€ä¸ªä¸éœ€è¦ï¼‰
        if idx < len(ALL_SCENARIOS) - 1:
            cooldown = 20
            print(f"\n  â³ åœºæ™¯å†·å´ {cooldown}s...")
            await asyncio.sleep(cooldown)
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    low_dims = generate_final_report(all_results)
    
    elapsed = time.time() - start_time
    print(f"\nâ±ï¸ æ€»è€—æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")
    
    failed_count = sum(1 for r in all_results if not r.passed)
    return failed_count, low_dims


if __name__ == "__main__":
    try:
        failed_count, low_dims = asyncio.run(main())
        sys.exit(1 if failed_count > 0 else 0)
    except KeyboardInterrupt:
        print("\nä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ è‡´å‘½é”™è¯¯: {e}")
        traceback.print_exc()
        sys.exit(2)
