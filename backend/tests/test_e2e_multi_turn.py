"""
ç«¯åˆ°ç«¯å¤šè½®å¯¹è¯æµ‹è¯• - çœŸå® LLM API è°ƒç”¨ + LLM-as-Judge è¯„ä¼°

ä¸ test_multi_turn.pyï¼ˆMock æµ‹è¯•ï¼‰ä¸åŒï¼Œè¿™ä¸ªæµ‹è¯•ï¼š
1. çœŸå®è°ƒç”¨ LLM API è¿›è¡Œå¤šè½®å¯¹è¯
2. ç”¨ LLM ä½œä¸ºè¯„åˆ¤è€…ï¼Œå¯¹æ¯è½®å›ç­”çš„è´¨é‡æ‰“åˆ†ï¼ˆ1-10åˆ†ï¼‰
3. è¯„ä¼°ç»´åº¦ï¼šä¸Šä¸‹æ–‡åˆ©ç”¨ã€æŒ‡ä»£è§£æã€ä¿¡æ¯å‡†ç¡®æ€§ã€å›ç­”è¿è´¯æ€§
4. å¦‚æœå‘ç°ä½åˆ†é¡¹ï¼Œè¾“å‡ºä¼˜åŒ–å»ºè®®

è¿è¡Œæ–¹å¼ï¼š
  cd backend && python tests/test_e2e_multi_turn.py
"""

import asyncio
import json
import sys
import os
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field

# é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), ".env"))

from llm.provider import LLMProviderFactory, LLMMessage, LLMConfig


# ============================================================
# è¯„ä¼°æ¡†æ¶
# ============================================================

@dataclass
class TurnResult:
    """å•è½®å¯¹è¯ç»“æœ"""
    turn_index: int
    user_query: str
    assistant_response: str
    latency_seconds: float
    # è¯„ä¼°ç»“æœ
    scores: Dict[str, int] = field(default_factory=dict)  # ç»´åº¦ â†’ åˆ†æ•°(1-10)
    evaluation_reasoning: str = ""
    

@dataclass 
class ScenarioResult:
    """å•ä¸ªåœºæ™¯çš„å®Œæ•´ç»“æœ"""
    name: str
    description: str
    turns: List[TurnResult] = field(default_factory=list)
    overall_score: float = 0.0
    evaluation_summary: str = ""
    errors: List[str] = field(default_factory=list)

    @property
    def passed(self) -> bool:
        return len(self.errors) == 0 and self.overall_score >= 6.0


# ============================================================
# çœŸå® LLM å¤šè½®å¯¹è¯å¼•æ“ï¼ˆè½»é‡ç‰ˆ DirectAgentï¼Œä¸ä¾èµ– skillsï¼‰
# ============================================================

class E2EConversationEngine:
    """ç«¯åˆ°ç«¯å¯¹è¯å¼•æ“ - ä½¿ç”¨çœŸå® LLMï¼Œæ¨¡æ‹Ÿ DirectAgent çš„æ ¸å¿ƒé€»è¾‘"""
    
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ AI åŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

## æ ¸å¿ƒèƒ½åŠ›
1. **æ·±åº¦åˆ†æ**ï¼šèƒ½å¤Ÿæ·±å…¥åˆ†æå¤æ‚é—®é¢˜ï¼Œæä¾›å…¨é¢ã€ä¸“ä¸šçš„è§è§£
2. **è®°å¿†ç³»ç»Ÿ**ï¼šèƒ½è®°ä½ç”¨æˆ·çš„åå¥½å’Œå†å²äº¤äº’

## å·¥ä½œåŸåˆ™
- ç›´æ¥ã€æ¸…æ™°åœ°å›ç­”ç”¨æˆ·é—®é¢˜
- ä½¿ç”¨ Markdown æ ¼å¼ç»„ç»‡è¾“å‡º
- æä¾›æœ‰æ·±åº¦å’Œå®ç”¨ä»·å€¼çš„å›ç­”

## å¤šè½®å¯¹è¯
ä½ æ­£å¤„äºä¸€ä¸ªè¿ç»­çš„å¤šè½®å¯¹è¯ä¸­ã€‚å¯¹è¯å†å²åŒ…å«äº†ä¹‹å‰æ‰€æœ‰è½®æ¬¡çš„å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- ç”¨æˆ·çš„æ¯ä¸€è½®æé—®
- ä½ çš„å›å¤å†…å®¹

**é‡è¦è§„åˆ™ï¼š**
1. **ä¸»åŠ¨å¼•ç”¨å†å²**ï¼šå›ç­”è¿½é—®æ—¶ï¼Œåº”ä¸»åŠ¨å¼•ç”¨ä½ ä¹‹å‰å›å¤ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆå¦‚å…·ä½“æ•°æ®ã€åˆ—è¡¨é¡¹ã€ç»“è®ºç­‰ï¼‰ï¼Œç”¨"æ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„..."æˆ–"åŸºäºå‰é¢è®¨è®ºçš„..."ç­‰æ–¹å¼å»ºç«‹è¿è´¯æ€§ï¼Œè®©ç”¨æˆ·æ„Ÿå—åˆ°ä½ å®Œæ•´è®°å¾—å¯¹è¯å†…å®¹ã€‚
2. **ç²¾ç¡®æŒ‡ä»£è§£æ**ï¼šå½“ç”¨æˆ·ä½¿ç”¨ä»£è¯ï¼ˆ"å®ƒ"ã€"é‚£ä¸ª"ã€"åè€…"ï¼‰ã€åºå·å¼•ç”¨ï¼ˆ"ç¬¬3ä¸ª"ã€"ç¬¬ä¸€æœ¬"ï¼‰æˆ–å›æŒ‡è¡¨è¾¾ï¼ˆ"ä½ åˆšè¯´çš„"ã€"ä¸Šé¢çš„"ï¼‰æ—¶ï¼Œå¿…é¡»å›æº¯å¯¹è¯å†å²ç²¾ç¡®å®šä½æŒ‡ä»£å¯¹è±¡ï¼Œä¸å¯çŒœæµ‹æˆ–æ³›æ³›å›ç­”ã€‚
3. **é€’è¿›å¼å±•å¼€**ï¼šå½“ç”¨æˆ·åœ¨å‰å‡ è½®è®¨è®ºçš„åŸºç¡€ä¸Šæ·±å…¥è¿½é—®æ—¶ï¼Œåº”åœ¨å‰æ–‡åŸºç¡€ä¸Šé€’è¿›å±•å¼€ï¼Œé¿å…é‡å¤å·²è®²è¿‡çš„åŸºç¡€æ¦‚å¿µï¼Œä½“ç°å¯¹è¯çš„å±‚å±‚æ·±å…¥ã€‚
4. **çº é”™åè®¤çŸ¥æ›´æ–°**ï¼šå¦‚æœç”¨æˆ·çº æ­£äº†ä½ çš„æŸä¸ªå›ç­”ï¼Œä½ åº”æ˜ç¡®æ‰¿è®¤å¹¶ä¿®æ­£ï¼Œåç»­å›å¤ä¸­å¿…é¡»ä½¿ç”¨ä¿®æ­£åçš„æ­£ç¡®ä¿¡æ¯ï¼Œä¸å¯é‡å¤é”™è¯¯ã€‚
5. å›ç­”ä¸­åº”ä½“ç°ä½ å¯¹ä¹‹å‰å¯¹è¯çš„è®°å¿†ï¼Œé€‚å½“å¼•ç”¨å‰é¢è®¨è®ºè¿‡çš„å…³é”®ä¿¡æ¯ã€‚"""
    
    def __init__(self, max_rounds: int = 6):
        self.provider = LLMProviderFactory.get_provider("openai")
        self.config = LLMProviderFactory.get_default_config("openai")
        self.config.temperature = 0.3  # é™ä½éšæœºæ€§ï¼Œè®©æµ‹è¯•æ›´ç¨³å®š
        self.config.max_tokens = 1024  # æ§åˆ¶å›å¤é•¿åº¦ï¼ŒåŠ é€Ÿæµ‹è¯•
        self.conversation_history: List[LLMMessage] = []
        self.max_rounds = max_rounds
    
    async def chat(self, user_input: str) -> str:
        """å‘é€ä¸€è½®å¯¹è¯ï¼Œè¿”å› LLM çš„å›å¤"""
        messages = [
            LLMMessage(role="system", content=self.SYSTEM_PROMPT),
            *self.conversation_history,
            LLMMessage(role="user", content=user_input),
        ]
        
        # ç”¨éæµå¼è°ƒç”¨ï¼ˆæ›´ç®€å•ï¼‰
        response = await self.provider.chat_complete(messages, self.config)
        assistant_reply = response.get("content", "")
        
        # æ›´æ–°å†å²
        self.conversation_history.append(LLMMessage(role="user", content=user_input))
        self.conversation_history.append(LLMMessage(role="assistant", content=assistant_reply))
        
        # è£å‰ª
        self._trim_history()
        
        return assistant_reply
    
    def _trim_history(self):
        """ä¸ DirectAgent ä¸€è‡´çš„è£å‰ªç­–ç•¥"""
        round_starts = [i for i, m in enumerate(self.conversation_history) if m.role == "user"]
        
        if len(round_starts) > self.max_rounds:
            trim_from = round_starts[-self.max_rounds]
            self.conversation_history = self.conversation_history[trim_from:]
            round_starts = [i for i, m in enumerate(self.conversation_history) if m.role == "user"]
        
        MAX_CHARS = 24000
        total_chars = sum(len(m.content or "") for m in self.conversation_history)
        while total_chars > MAX_CHARS and len(round_starts) > 2:
            next_start = round_starts[1] if len(round_starts) > 1 else len(self.conversation_history)
            removed = sum(len(m.content or "") for m in self.conversation_history[:next_start])
            self.conversation_history = self.conversation_history[next_start:]
            total_chars -= removed
            round_starts = [i for i, m in enumerate(self.conversation_history) if m.role == "user"]
    
    def reset(self):
        self.conversation_history.clear()


# ============================================================
# LLM-as-Judge è¯„ä¼°å™¨
# ============================================================

class LLMJudge:
    """ç”¨ LLM ä½œä¸ºè¯„åˆ¤è€…ï¼Œè¯„ä¼°å¤šè½®å¯¹è¯è´¨é‡"""
    
    JUDGE_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„å¤šè½®å¯¹è¯è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚ä½ éœ€è¦è¯„ä¼° AI åŠ©æ‰‹åœ¨å¤šè½®å¯¹è¯ä¸­çš„å›ç­”è´¨é‡ã€‚

## è¯„ä¼°ç»´åº¦ï¼ˆæ¯ä¸ªç»´åº¦ 1-10 åˆ†ï¼‰

1. **context_utilization**ï¼ˆä¸Šä¸‹æ–‡åˆ©ç”¨ï¼‰ï¼šå›ç­”æ˜¯å¦å……åˆ†åˆ©ç”¨äº†ä¹‹å‰å¯¹è¯ä¸­çš„ä¿¡æ¯ï¼Ÿæ˜¯å¦å¼•ç”¨äº†å†å²ä¸­çš„å…³é”®æ•°æ®ï¼Ÿ
2. **reference_resolution**ï¼ˆæŒ‡ä»£è§£æï¼‰ï¼šå½“ç”¨æˆ·ç”¨ä»£è¯ï¼ˆ"å®ƒ"ã€"é‚£ä¸ª"ã€"ä¸Šé¢çš„"ï¼‰æˆ–å›æŒ‡è¡¨è¾¾ï¼ˆ"ä½ åˆšè¯´çš„"ã€"ç¬¬ä¸€ä¸ª"ï¼‰æ—¶ï¼ŒAI æ˜¯å¦æ­£ç¡®è¯†åˆ«äº†æŒ‡ä»£å¯¹è±¡ï¼Ÿ
3. **information_accuracy**ï¼ˆä¿¡æ¯å‡†ç¡®æ€§ï¼‰ï¼šå›ç­”ä¸­å¼•ç”¨çš„å‰æ–‡ä¿¡æ¯æ˜¯å¦å‡†ç¡®ï¼Ÿæœ‰æ²¡æœ‰ç¼–é€ æˆ–æ··æ·†ä¹‹å‰çš„å†…å®¹ï¼Ÿ
4. **coherence**ï¼ˆè¿è´¯æ€§ï¼‰ï¼šå›ç­”æ˜¯å¦ä¸ä¹‹å‰çš„å¯¹è¯å†…å®¹é€»è¾‘ä¸€è‡´ï¼Ÿæ˜¯å¦å­˜åœ¨è‡ªç›¸çŸ›ç›¾ï¼Ÿ
5. **helpfulness**ï¼ˆæœ‰ç”¨æ€§ï¼‰ï¼šå›ç­”æ˜¯å¦æœ‰å®é™…å¸®åŠ©ï¼Ÿå†…å®¹æ˜¯å¦å……å®è€Œéæ•·è¡ï¼Ÿ

## è¯„åˆ†æ ‡å‡†
- 9-10: ä¼˜ç§€ï¼Œå®Œç¾åˆ©ç”¨å†å²ä¿¡æ¯ï¼Œå‡†ç¡®æŒ‡ä»£ï¼Œå†…å®¹ä¸°å¯Œ
- 7-8: è‰¯å¥½ï¼Œå¤§éƒ¨åˆ†åˆ©ç”¨äº†å†å²ï¼Œå¶æœ‰é—æ¼
- 5-6: åŠæ ¼ï¼ŒåŸºæœ¬å›ç­”äº†é—®é¢˜ä½†æœªå……åˆ†åˆ©ç”¨å†å²
- 3-4: å·®ï¼Œæ˜æ˜¾å¿½ç•¥äº†å†å²ä¿¡æ¯æˆ–æŒ‡ä»£é”™è¯¯
- 1-2: å¾ˆå·®ï¼Œå®Œå…¨æ²¡æœ‰åˆ©ç”¨å†å²ï¼Œå›ç­”ä¸ä¸Šä¸‹æ–‡è„±èŠ‚

## è¾“å‡ºæ ¼å¼
ä¸¥æ ¼è¾“å‡º JSONï¼ˆä¸è¦åŒ…å«```jsonæ ‡è®°ï¼‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
    "scores": {
        "context_utilization": <int>,
        "reference_resolution": <int>,
        "information_accuracy": <int>,
        "coherence": <int>,
        "helpfulness": <int>
    },
    "reasoning": "<ä¸€æ®µè¯„ä¼°ç†ç”±ï¼ŒæŒ‡å‡ºä¼˜ç‚¹å’Œä¸è¶³>",
    "improvement_suggestions": "<å¦‚æœåˆ†æ•°ä½äº7åˆ†ï¼Œç»™å‡ºå…·ä½“æ”¹è¿›å»ºè®®>"
}"""
    
    def __init__(self):
        self.provider = LLMProviderFactory.get_provider("openai")
        self.config = LLMProviderFactory.get_default_config("openai")
        self.config.temperature = 0.1  # è¯„ä¼°éœ€è¦é«˜ç¡®å®šæ€§
        self.config.max_tokens = 1024
    
    async def evaluate_turn(self, conversation_so_far: List[Dict[str, str]], 
                            current_query: str, current_response: str,
                            evaluation_focus: str = "") -> Dict[str, Any]:
        """è¯„ä¼°å•è½®å›ç­”è´¨é‡
        
        Args:
            conversation_so_far: ä¹‹å‰çš„å¯¹è¯ [{"role": "user/assistant", "content": "..."}]
            current_query: å½“å‰ç”¨æˆ·æé—®
            current_response: å½“å‰ AI å›å¤
            evaluation_focus: æœ¬è½®è¯„ä¼°çš„é‡ç‚¹è¯´æ˜
        
        Returns:
            {"scores": {...}, "reasoning": "...", "improvement_suggestions": "..."}
        """
        # æ„å»ºå¯¹è¯å†å²æ‘˜è¦
        history_text = ""
        for i, msg in enumerate(conversation_so_far):
            role_label = "ç”¨æˆ·" if msg["role"] == "user" else "AIåŠ©æ‰‹"
            history_text += f"ã€{role_label}ã€‘{msg['content']}\n\n"
        
        eval_prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹å¤šè½®å¯¹è¯ä¸­ï¼ŒAI åŠ©æ‰‹æœ€åä¸€è½®å›å¤çš„è´¨é‡ã€‚

## å¯¹è¯å†å²
{history_text}

## å½“å‰è½®æ¬¡
ã€ç”¨æˆ·ã€‘{current_query}

ã€AIåŠ©æ‰‹çš„å›å¤ï¼ˆå¾…è¯„ä¼°ï¼‰ã€‘
{current_response}

## è¯„ä¼°é‡ç‚¹
{evaluation_focus if evaluation_focus else "è¯·å…¨é¢è¯„ä¼°ä¸Šè¿°5ä¸ªç»´åº¦ã€‚"}

è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœã€‚"""
        
        messages = [
            LLMMessage(role="system", content=self.JUDGE_PROMPT),
            LLMMessage(role="user", content=eval_prompt),
        ]
        
        response = await self.provider.chat_complete(messages, self.config)
        content = response.get("content", "")
        
        # è§£æ JSONï¼ˆå¤šé‡å®¹é”™ï¼‰
        try:
            content = content.strip()
            
            # å»é™¤ markdown ä»£ç å—æ ‡è®°
            if content.startswith("```"):
                content = content.split("\n", 1)[1] if "\n" in content else content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            # å°è¯•ç›´æ¥è§£æ
            try:
                result = json.loads(content)
                return result
            except json.JSONDecodeError:
                pass
            
            # å°è¯•æå– JSON å—ï¼ˆæ­£åˆ™åŒ¹é…æœ€å¤–å±‚ {}ï¼‰
            import re
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                    return result
                except json.JSONDecodeError:
                    pass
            
            # å°è¯•ä¿®å¤å¸¸è§é—®é¢˜ï¼šä¸­æ–‡å¼•å·ã€å°¾éƒ¨é€—å·ã€æ§åˆ¶å­—ç¬¦
            cleaned = content
            cleaned = cleaned.replace('\u201c', '"').replace('\u201d', '"')  # ä¸­æ–‡å¼•å·
            cleaned = cleaned.replace('\u2018', "'").replace('\u2019', "'")
            cleaned = re.sub(r',\s*}', '}', cleaned)  # å°¾éƒ¨é€—å·
            cleaned = re.sub(r',\s*]', ']', cleaned)
            cleaned = re.sub(r'[\x00-\x1f\x7f]', ' ', cleaned)  # æ§åˆ¶å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
            cleaned = cleaned.replace('\n', ' ').replace('\r', ' ')
            
            json_match = re.search(r'\{[\s\S]*\}', cleaned)
            if json_match:
                result = json.loads(json_match.group())
                return result
            
            raise json.JSONDecodeError("æ— æ³•æå– JSON", content, 0)
            
        except (json.JSONDecodeError, KeyError) as e:
            print(f"  [Judge] JSON è§£æå¤±è´¥: {e}")
            print(f"  [Judge] åŸå§‹è¾“å‡º: {content[:500]}")
            return {
                "scores": {"context_utilization": 5, "reference_resolution": 5, 
                          "information_accuracy": 5, "coherence": 5, "helpfulness": 5},
                "reasoning": f"è¯„ä¼°è§£æå¤±è´¥: {str(e)}",
                "improvement_suggestions": ""
            }


# ============================================================
# æµ‹è¯•åœºæ™¯å®šä¹‰
# ============================================================

@dataclass
class TestTurn:
    """ä¸€è½®æµ‹è¯•çš„å®šä¹‰"""
    query: str
    evaluation_focus: str = ""  # è¯„ä¼°é‡ç‚¹
    min_expected_score: float = 6.0  # æœ€ä½æœŸæœ›åˆ†æ•°


@dataclass
class TestScenario:
    """æµ‹è¯•åœºæ™¯å®šä¹‰"""
    name: str
    description: str
    turns: List[TestTurn]


# åœºæ™¯1ï¼šé€’è¿›å¼çŸ¥è¯†è¿½é—®
SCENARIO_1 = TestScenario(
    name="é€’è¿›å¼çŸ¥è¯†è¿½é—®",
    description="å›´ç»•ä¸€ä¸ªä¸»é¢˜å±‚å±‚æ·±å…¥ï¼Œæµ‹è¯• LLM æ˜¯å¦èƒ½åˆ©ç”¨å‰æ–‡å›ç­”åšé€’è¿›å¼å±•å¼€",
    turns=[
        TestTurn(
            query="è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹ Transformer æ¶æ„çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†",
            evaluation_focus="é¦–è½®å›å¤ï¼Œå…³æ³¨å†…å®¹çš„å‡†ç¡®æ€§å’Œå®Œæ•´åº¦",
        ),
        TestTurn(
            query="ä½ æåˆ°çš„ Self-Attention æœºåˆ¶ï¼Œèƒ½ç”¨é€šä¿—çš„æ¯”å–»è§£é‡Šä¸€ä¸‹å—ï¼Ÿ",
            evaluation_focus="å…³é”®éªŒè¯ï¼šLLM æ˜¯å¦èƒ½è¯†åˆ« 'Self-Attention' æ¥è‡ªä¸Šä¸€è½®å›å¤ï¼Œè€Œéæ³›æ³›å›ç­”",
        ),
        TestTurn(
            query="é‚£ Multi-Head çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿå’Œå•ä¸ª Head æ¯”æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
            evaluation_focus="éªŒè¯æ˜¯å¦åœ¨å‰ä¸¤è½®çš„åŸºç¡€ä¸Šåšé€’è¿›ï¼Œè€Œéé‡å¤è§£é‡ŠåŸºç¡€æ¦‚å¿µ",
        ),
        TestTurn(
            query="ç»“åˆä½ å‰é¢çš„è§£é‡Šï¼Œä¸ºä»€ä¹ˆè¯´ Transformer æ¯” RNN æ›´é€‚åˆå¹¶è¡Œè®¡ç®—ï¼Ÿ",
            evaluation_focus="æ ¸å¿ƒéªŒè¯ï¼šéœ€è¦ç»¼åˆå‰3è½®è®¨è®ºçš„ attentionã€multi-head ç­‰å†…å®¹æ¥å›ç­”",
            min_expected_score=7.0,
        ),
    ]
)

# åœºæ™¯2ï¼šä»£è¯æŒ‡ä»£ä¸è¯é¢˜è·³è½¬
SCENARIO_2 = TestScenario(
    name="ä»£è¯æŒ‡ä»£ä¸è¯é¢˜è·³è½¬",
    description="é¢‘ç¹ä½¿ç”¨ä»£è¯æŒ‡ä»£å’Œè¯é¢˜åˆ‡æ¢ï¼Œæµ‹è¯•ä¸Šä¸‹æ–‡è¿½è¸ªèƒ½åŠ›",
    turns=[
        TestTurn(
            query="Python å’Œ Rust è¿™ä¸¤ä¸ªè¯­è¨€å„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ",
            evaluation_focus="é¦–è½®ï¼Œå…³æ³¨æ˜¯å¦æ¸…æ™°åˆ—ä¸¾äº†ä¸¤ç§è¯­è¨€çš„ä¼˜ç¼ºç‚¹",
        ),
        TestTurn(
            query="å®ƒä»¬åœ¨ Web åç«¯å¼€å‘ä¸­åˆ†åˆ«é€‚åˆä»€ä¹ˆåœºæ™¯ï¼Ÿ",
            evaluation_focus="éªŒè¯ 'å®ƒä»¬' æ˜¯å¦æ­£ç¡®è§£æä¸º Python å’Œ Rust",
        ),
        TestTurn(
            query="åè€…åœ¨å†…å­˜å®‰å…¨æ–¹é¢çš„è®¾è®¡ç†å¿µæ˜¯ä»€ä¹ˆï¼Ÿ",
            evaluation_focus="æ ¸å¿ƒéªŒè¯ï¼š'åè€…' æ˜¯å¦æ­£ç¡®è§£æä¸º Rustï¼ˆè€Œé Pythonï¼‰",
            min_expected_score=7.0,
        ),
        TestTurn(
            query="å¯¹äº†ï¼Œä½ ä¸€å¼€å§‹æåˆ°çš„ Python çš„ä¸»è¦ç¼ºç‚¹æ˜¯ä»€ä¹ˆæ¥ç€ï¼Ÿ",
            evaluation_focus="å›æº¯éªŒè¯ï¼šæ˜¯å¦èƒ½å‡†ç¡®å¼•ç”¨ç¬¬1è½®ä¸­å…³äº Python ç¼ºç‚¹çš„å†…å®¹",
            min_expected_score=7.0,
        ),
    ]
)

# åœºæ™¯3ï¼šæ•°å­—ä¸åˆ—è¡¨å¼•ç”¨
SCENARIO_3 = TestScenario(
    name="æ•°å­—ä¸åˆ—è¡¨å¼•ç”¨",
    description="LLM å›å¤äº†ç¼–å·åˆ—è¡¨ï¼Œåç»­é€šè¿‡åºå·å¼•ç”¨ï¼Œæµ‹è¯•ç²¾ç¡®å¼•ç”¨èƒ½åŠ›",
    turns=[
        TestTurn(
            query="æ¨è5æœ¬è®¡ç®—æœºç§‘å­¦ç»å…¸ä¹¦ç±ï¼Œè¯·ç¼–å·åˆ—å‡ºï¼ŒåŒ…å«ä¹¦åã€ä½œè€…å’Œä¸€å¥è¯æ¨èç†ç”±",
            evaluation_focus="é¦–è½®ï¼ŒéªŒè¯æ˜¯å¦è¿”å›äº†ç¼–å·åˆ—è¡¨ä¸”ä¿¡æ¯å®Œæ•´",
        ),
        TestTurn(
            query="ç¬¬3æœ¬é€‚åˆä»€ä¹ˆæ°´å¹³çš„è¯»è€…ï¼Ÿ",
            evaluation_focus="æ ¸å¿ƒéªŒè¯ï¼šæ˜¯å¦å‡†ç¡®å®šä½åˆ°ç¼–å·ç¬¬3æœ¬ä¹¦ï¼Œè€Œééšæ„é€‰æ‹©",
            min_expected_score=7.0,
        ),
        TestTurn(
            query="æŠŠç¬¬1æœ¬å’Œç¬¬5æœ¬åšä¸ªå¯¹æ¯”ï¼Œå“ªæœ¬æ›´é€‚åˆä½œä¸ºå…¥é—¨è¯»ç‰©ï¼Ÿ",
            evaluation_focus="éªŒè¯æ˜¯å¦å‡†ç¡®å¼•ç”¨äº†ç¬¬1æœ¬å’Œç¬¬5æœ¬çš„å…·ä½“ä¿¡æ¯",
            min_expected_score=7.0,
        ),
        TestTurn(
            query="ä½ æ¨èçš„è¿™5æœ¬ä¸­ï¼Œå“ªæœ¬æœ€è–„ï¼Ÿå¤§æ¦‚å¤šå°‘é¡µï¼Ÿ",
            evaluation_focus="ç»¼åˆéªŒè¯ï¼šéœ€è¦å›å¿†æ‰€æœ‰5æœ¬ä¹¦æ¥åšæ¯”è¾ƒ",
        ),
    ]
)

# åœºæ™¯4ï¼šçº é”™ä¸ä¿¡æ¯ä¿®æ­£
SCENARIO_4 = TestScenario(
    name="çº é”™ä¸ä¿¡æ¯ä¿®æ­£",
    description="ç”¨æˆ·åœ¨å¯¹è¯ä¸­çº æ­£ AI çš„é”™è¯¯ï¼Œæµ‹è¯• AI æ˜¯å¦èƒ½æ­£ç¡®æ›´æ–°è®¤çŸ¥",
    turns=[
        TestTurn(
            query="Linux æ˜¯è°å‘æ˜çš„ï¼Ÿä»€ä¹ˆæ—¶å€™å‘å¸ƒçš„ç¬¬ä¸€ä¸ªç‰ˆæœ¬ï¼Ÿ",
            evaluation_focus="é¦–è½®äº‹å®æ€§é—®é¢˜",
        ),
        TestTurn(
            query="ä½ è¯´å¾—å¯¹ã€‚é‚£ Git ä¹Ÿæ˜¯ä»–å‘æ˜çš„å—ï¼Ÿæ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ",
            evaluation_focus="éªŒè¯ 'ä»–' æ˜¯å¦æ­£ç¡®æŒ‡ä»£ Linus Torvalds",
        ),
        TestTurn(
            query="ä¸å¯¹ï¼Œæˆ‘è®°å¾— Git æ˜¯2005å¹´å‘å¸ƒçš„ã€‚ä½ ç¡®è®¤ä¸€ä¸‹ï¼Ÿ",
            evaluation_focus="éªŒè¯ AI è¢«çº é”™åæ˜¯å¦èƒ½æ­£ç¡®è°ƒæ•´å›ç­”",
            min_expected_score=7.0,
        ),
        TestTurn(
            query="å¥½çš„ï¼Œé‚£æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬èŠåˆ°çš„ Linus çš„ä¸¤ä¸ªä¸»è¦ä½œå“å’Œå®ƒä»¬çš„å‘å¸ƒæ—¶é—´",
            evaluation_focus="ç»¼åˆéªŒè¯ï¼šéœ€è¦å‡†ç¡®å¼•ç”¨å‰é¢è®¨è®ºçš„ä¿¡æ¯ï¼Œä¸”åº”åæ˜ çº é”™åçš„æ­£ç¡®ä¿¡æ¯",
            min_expected_score=7.0,
        ),
    ]
)

# åœºæ™¯5ï¼šé•¿è·ç¦»ä¿¡æ¯ä¿æŒ
SCENARIO_5 = TestScenario(
    name="é•¿è·ç¦»ä¿¡æ¯ä¿æŒ",
    description="5è½®å¯¹è¯åä»éœ€å¼•ç”¨ç¬¬1è½®çš„å…·ä½“ä¿¡æ¯ï¼Œæµ‹è¯•å†å²çª—å£æœ‰æ•ˆæ€§",
    turns=[
        TestTurn(
            query="ç»™æˆ‘åˆ—å‡ºä¸­å›½å››å¤§å‘æ˜ï¼Œä»¥åŠæ¯ä¸ªå‘æ˜å¤§çº¦æ˜¯ä»€ä¹ˆæœä»£å‡ºç°çš„",
            evaluation_focus="é¦–è½®åŸºç¡€çŸ¥è¯†é—®é¢˜",
        ),
        TestTurn(
            query="å…¶ä¸­é€ çº¸æœ¯å¯¹ä¸–ç•Œæ–‡æ˜æœ‰ä»€ä¹ˆé‡å¤§å½±å“ï¼Ÿ",
            evaluation_focus="è¿½é—®å•é¡¹",
        ),
        TestTurn(
            query="ç«è¯å‘¢ï¼Ÿå®ƒæœ€åˆæ˜¯ç”¨æ¥åšä»€ä¹ˆçš„ï¼Ÿ",
            evaluation_focus="åˆ‡æ¢åˆ°å¦ä¸€é¡¹",
        ),
        TestTurn(
            query="æŒ‡å—é’ˆå¯¹èˆªæµ·æœ‰ä»€ä¹ˆé‡è¦æ„ä¹‰ï¼Ÿ",
            evaluation_focus="å†åˆ‡æ¢ä¸€é¡¹",
        ),
        TestTurn(
            query="å›åˆ°æœ€å¼€å§‹çš„é—®é¢˜ï¼Œä½ åˆ—å‡ºçš„å››å¤§å‘æ˜åˆ†åˆ«å¯¹åº”ä»€ä¹ˆæœä»£æ¥ç€ï¼Ÿå¸®æˆ‘é‡æ–°ç¡®è®¤ä¸€ä¸‹",
            evaluation_focus="é•¿è·ç¦»å›æº¯ï¼šéœ€è¦å‡†ç¡®å¼•ç”¨ç¬¬1è½®ç»™å‡ºçš„æœä»£ä¿¡æ¯",
            min_expected_score=7.0,
        ),
    ]
)

ALL_SCENARIOS = [SCENARIO_1, SCENARIO_2, SCENARIO_3, SCENARIO_4, SCENARIO_5]


# ============================================================
# æµ‹è¯•è¿è¡Œå™¨
# ============================================================

async def run_scenario(scenario: TestScenario, engine: E2EConversationEngine, 
                       judge: LLMJudge) -> ScenarioResult:
    """è¿è¡Œå•ä¸ªæµ‹è¯•åœºæ™¯"""
    result = ScenarioResult(name=scenario.name, description=scenario.description)
    
    conversation_so_far: List[Dict[str, str]] = []
    
    for i, turn in enumerate(scenario.turns):
        print(f"    è½®æ¬¡ {i+1}/{len(scenario.turns)}: {turn.query[:40]}...")
        
        # çœŸå® LLM å¯¹è¯
        start_time = time.time()
        try:
            response = await engine.chat(turn.query)
        except Exception as e:
            result.errors.append(f"è½®æ¬¡{i+1} LLM è°ƒç”¨å¤±è´¥: {str(e)}")
            print(f"      âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
            continue
        latency = time.time() - start_time
        
        print(f"      å›å¤ ({latency:.1f}s): {response[:80]}...")
        
        # LLM-as-Judge è¯„ä¼°ï¼ˆè·³è¿‡é¦–è½®çš„æŒ‡ä»£è§£æç»´åº¦ï¼‰
        try:
            eval_result = await judge.evaluate_turn(
                conversation_so_far=conversation_so_far,
                current_query=turn.query,
                current_response=response,
                evaluation_focus=turn.evaluation_focus,
            )
        except Exception as e:
            print(f"      âš ï¸ è¯„ä¼°å¤±è´¥: {e}")
            eval_result = {
                "scores": {"context_utilization": 5, "reference_resolution": 5, 
                          "information_accuracy": 5, "coherence": 5, "helpfulness": 5},
                "reasoning": f"è¯„ä¼°è°ƒç”¨å¤±è´¥: {str(e)}",
                "improvement_suggestions": ""
            }
        
        scores = eval_result.get("scores", {})
        avg_score = sum(scores.values()) / len(scores) if scores else 0
        
        turn_result = TurnResult(
            turn_index=i + 1,
            user_query=turn.query,
            assistant_response=response,
            latency_seconds=latency,
            scores=scores,
            evaluation_reasoning=eval_result.get("reasoning", ""),
        )
        result.turns.append(turn_result)
        
        # æ‰“å°è¯„åˆ†
        score_str = " | ".join(f"{k}:{v}" for k, v in scores.items())
        print(f"      è¯„åˆ† [avg={avg_score:.1f}]: {score_str}")
        
        # æ£€æŸ¥æ˜¯å¦ä½äºé¢„æœŸ
        if avg_score < turn.min_expected_score:
            msg = f"è½®æ¬¡{i+1} å¹³å‡åˆ† {avg_score:.1f} ä½äºé¢„æœŸ {turn.min_expected_score}"
            result.errors.append(msg)
            print(f"      âš ï¸ {msg}")
            if eval_result.get("improvement_suggestions"):
                print(f"      ğŸ’¡ å»ºè®®: {eval_result['improvement_suggestions'][:200]}")
        
        # æ›´æ–°å¯¹è¯å†å²ï¼ˆç»™ Judge ç”¨ï¼‰
        conversation_so_far.append({"role": "user", "content": turn.query})
        conversation_so_far.append({"role": "assistant", "content": response})
    
    # è®¡ç®—åœºæ™¯æ€»åˆ†
    all_scores = []
    for t in result.turns:
        if t.scores:
            all_scores.extend(t.scores.values())
    result.overall_score = sum(all_scores) / len(all_scores) if all_scores else 0
    
    return result


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•"""
    print("=" * 70)
    print("ç«¯åˆ°ç«¯å¤šè½®å¯¹è¯æµ‹è¯• - çœŸå® LLM + LLM-as-Judge è¯„ä¼°")
    print("=" * 70)
    print()
    
    judge = LLMJudge()
    all_results: List[ScenarioResult] = []
    
    for idx, scenario in enumerate(ALL_SCENARIOS):
        engine = E2EConversationEngine()  # æ¯ä¸ªåœºæ™¯ç‹¬ç«‹çš„å¯¹è¯å¼•æ“
        print(f"  [{idx+1}/{len(ALL_SCENARIOS)}] åœºæ™¯: {scenario.name}")
        print(f"  æè¿°: {scenario.description}")
        print()
        
        result = await run_scenario(scenario, engine, judge)
        all_results.append(result)
        
        status = "âœ… PASS" if result.passed else "âŒ FAIL"
        print(f"\n  {status} {result.name} (æ€»åˆ†: {result.overall_score:.1f}/10)")
        if result.errors:
            for e in result.errors:
                print(f"    âš ï¸ {e}")
        print()
        print("-" * 70)
        print()
    
    # ============================================================
    # æ±‡æ€»æŠ¥å‘Š
    # ============================================================
    print()
    print("=" * 70)
    print("è¯„ä¼°æ±‡æ€»æŠ¥å‘Š")
    print("=" * 70)
    print()
    
    # æŒ‰ç»´åº¦æ±‡æ€»
    dimension_scores: Dict[str, List[int]] = {}
    for result in all_results:
        for turn in result.turns:
            for dim, score in turn.scores.items():
                dimension_scores.setdefault(dim, []).append(score)
    
    print("å„ç»´åº¦å¹³å‡åˆ†:")
    low_dimensions = []
    for dim, scores in sorted(dimension_scores.items()):
        avg = sum(scores) / len(scores)
        bar = "â–ˆ" * int(avg) + "â–‘" * (10 - int(avg))
        status = "âœ…" if avg >= 7 else ("âš ï¸" if avg >= 5 else "âŒ")
        print(f"  {status} {dim:30s} {bar} {avg:.1f}/10 (n={len(scores)})")
        if avg < 7:
            low_dimensions.append((dim, avg))
    
    print()
    
    # åœºæ™¯æ±‡æ€»
    print("å„åœºæ™¯æ€»åˆ†:")
    passed = 0
    failed = 0
    for result in all_results:
        status = "âœ…" if result.passed else "âŒ"
        print(f"  {status} {result.name:30s} {result.overall_score:.1f}/10")
        if result.passed:
            passed += 1
        else:
            failed += 1
    
    total = passed + failed
    print(f"\næµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡, {failed} å¤±è´¥")
    
    # ä½åˆ†ç»´åº¦åˆ†æ
    if low_dimensions:
        print()
        print("=" * 70)
        print("âš ï¸ ä½åˆ†ç»´åº¦åˆ†æä¸ä¼˜åŒ–å»ºè®®")
        print("=" * 70)
        for dim, avg in low_dimensions:
            print(f"\n  [{dim}] å¹³å‡åˆ†: {avg:.1f}")
            # æ‰¾å‡ºè¯¥ç»´åº¦æœ€ä½åˆ†çš„å…·ä½“è½®æ¬¡
            worst_turns = []
            for result in all_results:
                for turn in result.turns:
                    if dim in turn.scores and turn.scores[dim] < 7:
                        worst_turns.append((result.name, turn))
            for scenario_name, turn in worst_turns[:3]:
                print(f"    - åœºæ™¯ã€Œ{scenario_name}ã€è½®æ¬¡{turn.turn_index}: score={turn.scores.get(dim, '?')}")
                print(f"      é—®: {turn.user_query[:60]}")
                print(f"      ç­”: {turn.assistant_response[:80]}...")
                if turn.evaluation_reasoning:
                    print(f"      è¯„è¯­: {turn.evaluation_reasoning[:150]}...")
    
    # è¾“å‡ºåŸå§‹æ•°æ®ä¾›è¿›ä¸€æ­¥åˆ†æ
    print()
    print("=" * 70)
    print("è¯¦ç»†è¯„åˆ†æ•°æ®")
    print("=" * 70)
    for result in all_results:
        print(f"\n  åœºæ™¯: {result.name} (æ€»åˆ†: {result.overall_score:.1f})")
        for turn in result.turns:
            scores_str = ", ".join(f"{k}={v}" for k, v in turn.scores.items())
            avg = sum(turn.scores.values()) / len(turn.scores) if turn.scores else 0
            print(f"    Turn {turn.turn_index} [avg={avg:.1f}]: {scores_str}")
            print(f"      Q: {turn.user_query[:70]}")
            print(f"      A: {turn.assistant_response[:100]}...")
            if turn.evaluation_reasoning:
                print(f"      è¯„è¯­: {turn.evaluation_reasoning[:200]}")
    
    return all_results, low_dimensions


if __name__ == "__main__":
    results, low_dims = asyncio.run(run_all_tests())
    
    # é€€å‡ºç ï¼šæœ‰å¤±è´¥åˆ™è¿”å› 1
    failed_count = sum(1 for r in results if not r.passed)
    sys.exit(1 if failed_count > 0 else 0)
