"""
å¤šè½®å¯¹è¯æµ‹è¯•é›† - 10ä¸ªåœºæ™¯ï¼Œæ¯ä¸ªè‡³å°‘4è½®

æµ‹è¯• DirectAgent çš„ conversation_history ç®¡ç†é€»è¾‘ï¼š
1. tool calling é“¾æ˜¯å¦å®Œæ•´ä¿å­˜
2. è¿½é—®æ—¶èƒ½å¦å¼•ç”¨ä¸Šä¸€è½®çš„å·¥å…·ç»“æœ
3. æ™ºèƒ½è£å‰ªæ˜¯å¦æŒ‰è½®æ¬¡æ­£ç¡®è£å‰ª
4. æ¶ˆæ¯è§’è‰²åºåˆ—æ˜¯å¦åˆæ³•
5. extract_session_summary æ˜¯å¦å…¼å®¹æ–°ç»“æ„

è¿è¡Œæ–¹å¼ï¼š
  cd backend && python -m pytest tests/test_multi_turn.py -v
æˆ–
  cd backend && python tests/test_multi_turn.py
"""

import asyncio
import json
import sys
import os
from typing import List, Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass

# é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm.provider import LLMMessage, LLMConfig, LLMProvider


# ============================================================
# Mock LLM Provider - å¯ç¼–ç¨‹çš„ LLM å“åº”
# ============================================================

class MockLLMProvider(LLMProvider):
    """å¯ç¼–ç¨‹çš„ Mock LLM Providerï¼Œæ”¯æŒé¢„è®¾ tool calling å’Œæ–‡æœ¬å›å¤åºåˆ—
    
    å¢å¼ºåŠŸèƒ½ï¼š
    - call_log: è®°å½•æ¯æ¬¡è°ƒç”¨æ—¶å®Œæ•´çš„ messagesï¼Œç”¨äºäº‹åæ–­è¨€ä¸Šä¸‹æ–‡
    - all_call_log: è·¨ reset ä¿ç•™çš„å®Œæ•´è°ƒç”¨æ—¥å¿—ï¼ˆç”¨äºå¤šè½®æµ‹è¯•ï¼‰
    """
    
    def __init__(self):
        self._responses = []  # é¢„è®¾çš„å“åº”é˜Ÿåˆ—
        self._call_idx = 0
        self.call_log: List[List[LLMMessage]] = []  # å½“å‰è½®çš„è°ƒç”¨æ—¥å¿—
        self.all_call_log: List[List[LLMMessage]] = []  # è·¨ reset çš„å®Œæ•´è°ƒç”¨æ—¥å¿—
    
    def add_response(self, content: str = "", tool_calls: Optional[List[Dict]] = None):
        """æ·»åŠ ä¸€ä¸ªé¢„è®¾å“åº”ï¼ˆæŒ‰è°ƒç”¨é¡ºåºæ¶ˆè´¹ï¼‰"""
        self._responses.append({
            "content": content,
            "tool_calls": tool_calls,
            "finish_reason": "tool_calls" if tool_calls else "stop"
        })
    
    async def chat_complete(
        self,
        messages: List[LLMMessage],
        config: LLMConfig,
        tools: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        snapshot = list(messages)
        self.call_log.append(snapshot)
        self.all_call_log.append(snapshot)
        if self._call_idx < len(self._responses):
            resp = self._responses[self._call_idx]
            self._call_idx += 1
            return resp
        return {"content": "", "tool_calls": None, "finish_reason": "stop"}
    
    async def chat(
        self,
        messages: List[LLMMessage],
        config: LLMConfig,
        tools: Optional[List[Dict[str, Any]]] = None
    ) -> AsyncGenerator[str, None]:
        snapshot = list(messages)
        self.call_log.append(snapshot)
        self.all_call_log.append(snapshot)
        if self._call_idx < len(self._responses):
            resp = self._responses[self._call_idx]
            self._call_idx += 1
            content = resp.get("content", "")
            if content:
                for i in range(0, len(content), 20):
                    yield content[i:i+20]
        else:
            yield "[Mock] No more responses"
    
    def reset(self):
        """é‡ç½®å“åº”é˜Ÿåˆ—å’Œå½“å‰è½®æ—¥å¿—ï¼ˆä¿ç•™ all_call_logï¼‰"""
        self._responses.clear()
        self._call_idx = 0
        self.call_log.clear()
    
    def get_last_call_messages(self) -> List[LLMMessage]:
        """è·å–æœ€åä¸€æ¬¡ LLM è°ƒç”¨æ”¶åˆ°çš„ messages"""
        return self.all_call_log[-1] if self.all_call_log else []
    
    def get_last_call_context_text(self) -> str:
        """è·å–æœ€åä¸€æ¬¡ LLM è°ƒç”¨çš„å…¨éƒ¨ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆç”¨äºå…³é”®è¯æœç´¢ï¼‰"""
        msgs = self.get_last_call_messages()
        return " ".join(m.content or "" for m in msgs)


# ============================================================
# Mock SkillSet - å¯ç¼–ç¨‹çš„æŠ€èƒ½æ‰§è¡Œ
# ============================================================

@dataclass
class MockSkillResult:
    success: bool = True
    result: str = ""
    summary: str = ""
    error: Optional[str] = None

class MockSkillSet:
    """Mock SkillSetï¼Œè®©æˆ‘ä»¬æ§åˆ¶æŠ€èƒ½è¿”å›çš„æ•°æ®"""
    
    def __init__(self):
        self._results: Dict[str, MockSkillResult] = {}
    
    def set_result(self, skill_name: str, result: str, summary: str = ""):
        self._results[skill_name] = MockSkillResult(
            success=True, result=result, summary=summary or result[:80]
        )
    
    async def execute_skill(self, skill_name: str, **kwargs) -> MockSkillResult:
        return self._results.get(skill_name, MockSkillResult(success=False, error="Unknown skill"))
    
    def get_tool_definitions(self):
        return [
            {
                "type": "function",
                "function": {
                    "name": "web-search",
                    "description": "æœç´¢ç½‘ç»œ",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "task": {"type": "string", "description": "æœç´¢å…³é”®è¯"}
                        },
                        "required": ["task"]
                    }
                }
            }
        ]
    
    def list_skills(self):
        return list(self._results.keys())
    
    def assign_skills(self, names):
        return len(names)


# ============================================================
# è¾…åŠ©å‡½æ•°
# ============================================================

def build_tool_call(func_name: str, args: Dict, call_id: str = None) -> Dict:
    """æ„å»ºä¸€ä¸ª tool_call å­—å…¸"""
    return {
        "id": call_id or f"call_{func_name}_{id(args)}",
        "type": "function",
        "function": {
            "name": func_name,
            "arguments": json.dumps(args, ensure_ascii=False)
        }
    }

def count_roles(history: List[LLMMessage]) -> Dict[str, int]:
    """ç»Ÿè®¡ conversation_history ä¸­å„è§’è‰²çš„æ¶ˆæ¯æ•°"""
    counts = {}
    for m in history:
        counts[m.role] = counts.get(m.role, 0) + 1
    return counts

def count_rounds(history: List[LLMMessage]) -> int:
    """ç»Ÿè®¡å¯¹è¯è½®æ¬¡ï¼ˆuser æ¶ˆæ¯æ•°é‡ï¼‰"""
    return sum(1 for m in history if m.role == "user")

def assert_context_contains(provider: MockLLMProvider, keywords: List[str], 
                           call_index: int = -1, description: str = "") -> List[str]:
    """æ–­è¨€ LLM æ”¶åˆ°çš„ context ä¸­åŒ…å«æŒ‡å®šå…³é”®è¯
    
    è¿™æ˜¯ã€Œå›ç­”è´¨é‡ã€æµ‹è¯•çš„æ ¸å¿ƒï¼šå¦‚æœä¼ ç»™ LLM çš„ä¸Šä¸‹æ–‡åŒ…å«äº†æ­£ç¡®çš„å†å²ä¿¡æ¯ï¼Œ
    é‚£ä¹ˆä¸€ä¸ªåˆæ ¼çš„ LLM å°±åº”è¯¥èƒ½ç»™å‡ºæ­£ç¡®çš„å›ç­”ã€‚
    
    Args:
        provider: MockLLMProvider å®ä¾‹
        keywords: å¿…é¡»å‡ºç°çš„å…³é”®è¯åˆ—è¡¨
        call_index: æ£€æŸ¥ç¬¬å‡ æ¬¡è°ƒç”¨ï¼ˆ-1 ä¸ºæœ€åä¸€æ¬¡ï¼‰
        description: æè¿°ä¿¡æ¯
    
    Returns:
        é”™è¯¯åˆ—è¡¨ï¼ˆç©ºåˆ—è¡¨è¡¨ç¤ºå…¨éƒ¨é€šè¿‡ï¼‰
    """
    errors = []
    if not provider.all_call_log:
        errors.append(f"[{description}] LLM ä»æœªè¢«è°ƒç”¨")
        return errors
    
    try:
        messages = provider.all_call_log[call_index]
    except IndexError:
        errors.append(f"[{description}] è°ƒç”¨ç´¢å¼• {call_index} è¶…å‡ºèŒƒå›´ (å…± {len(provider.all_call_log)} æ¬¡è°ƒç”¨)")
        return errors
    
    context_text = " ".join(m.content or "" for m in messages)
    
    for kw in keywords:
        if kw not in context_text:
            errors.append(f"[{description}] LLM ä¸Šä¸‹æ–‡ä¸­ç¼ºå°‘å…³é”®è¯: '{kw}'")
    
    return errors


def assert_context_has_role(provider: MockLLMProvider, role: str, 
                            call_index: int = -1, description: str = "") -> List[str]:
    """æ–­è¨€ LLM æ”¶åˆ°çš„ messages ä¸­åŒ…å«æŒ‡å®šè§’è‰²çš„æ¶ˆæ¯"""
    errors = []
    if not provider.all_call_log:
        errors.append(f"[{description}] LLM ä»æœªè¢«è°ƒç”¨")
        return errors
    
    try:
        messages = provider.all_call_log[call_index]
    except IndexError:
        errors.append(f"[{description}] è°ƒç”¨ç´¢å¼• {call_index} è¶…å‡ºèŒƒå›´")
        return errors
    
    if not any(m.role == role for m in messages):
        errors.append(f"[{description}] LLM messages ä¸­ç¼ºå°‘ role='{role}' çš„æ¶ˆæ¯")
    
    return errors


def assert_response_quality(response: str, expected_keywords: List[str], 
                           description: str = "") -> List[str]:
    """æ–­è¨€ LLM çš„å›å¤ä¸­åŒ…å«é¢„æœŸçš„å…³é”®å†…å®¹
    
    åœ¨ Mock åœºæ™¯ä¸‹ï¼Œè¿™éªŒè¯çš„æ˜¯æˆ‘ä»¬é¢„è®¾çš„å›å¤æ˜¯å¦åˆç†ã€‚
    åœ¨çœŸå® LLM åœºæ™¯ä¸‹ï¼Œè¿™éªŒè¯çš„æ˜¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åˆ©ç”¨äº†ä¸Šä¸‹æ–‡ã€‚
    
    Args:
        response: LLM çš„å›å¤æ–‡æœ¬
        expected_keywords: å›å¤ä¸­åº”åŒ…å«çš„å…³é”®è¯
        description: æè¿°ä¿¡æ¯
    
    Returns:
        é”™è¯¯åˆ—è¡¨
    """
    errors = []
    for kw in expected_keywords:
        if kw not in response:
            errors.append(f"[{description}] å›å¤ä¸­ç¼ºå°‘é¢„æœŸå†…å®¹: '{kw}'")
    return errors


def validate_message_sequence(history: List[LLMMessage]) -> List[str]:
    """éªŒè¯æ¶ˆæ¯åºåˆ—çš„åˆæ³•æ€§ï¼Œè¿”å›é”™è¯¯åˆ—è¡¨"""
    errors = []
    if not history:
        return errors
    
    # ç¬¬ä¸€æ¡åº”è¯¥æ˜¯ user
    if history[0].role != "user":
        errors.append(f"ç¬¬ä¸€æ¡æ¶ˆæ¯åº”ä¸º userï¼Œå®é™…ä¸º {history[0].role}")
    
    for i, msg in enumerate(history):
        # tool æ¶ˆæ¯å‰å¿…é¡»æœ‰ assistant(tool_calls) æ¶ˆæ¯
        if msg.role == "tool":
            # å‘å‰æ‰¾æœ€è¿‘çš„ assistant æ¶ˆæ¯
            found_tc = False
            for j in range(i-1, -1, -1):
                if history[j].role == "assistant" and history[j].tool_calls:
                    found_tc = True
                    break
                if history[j].role == "user":
                    break
            if not found_tc:
                errors.append(f"ç¬¬ {i} æ¡ tool æ¶ˆæ¯å‰ç¼ºå°‘ assistant(tool_calls) æ¶ˆæ¯")
    
    return errors


# ============================================================
# ç›´æ¥æ¨¡æ‹Ÿ DirectAgent çš„æ ¸å¿ƒé€»è¾‘ï¼ˆä¸ä¾èµ–å®Œæ•´æ¡†æ¶ï¼‰
# ============================================================

class DirectAgentSimulator:
    """
    æ¨¡æ‹Ÿ DirectAgent çš„ conversation_history ç®¡ç†é€»è¾‘ï¼Œ
    ç”¨äºæµ‹è¯•è€Œä¸éœ€è¦å®Œæ•´çš„æ¡†æ¶ä¾èµ–ï¼ˆå¦‚ skillsã€memoryã€events ç­‰ï¼‰ã€‚
    
    è¿™é‡Œå¤ç°äº† execute_task ä¸­æ¶ˆæ¯æ„å»ºå’Œä¿å­˜çš„æ ¸å¿ƒé€»è¾‘ã€‚
    """
    
    def __init__(self, provider: MockLLMProvider, skill_set: MockSkillSet):
        self.provider = provider
        self.skill_set = skill_set
        self.conversation_history: List[LLMMessage] = []
        self.llm_config = LLMConfig(model="mock-model")
    
    async def execute_task(self, task: str) -> str:
        """æ¨¡æ‹Ÿ execute_task çš„æ ¸å¿ƒé€»è¾‘"""
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ­£å¤„äºå¤šè½®å¯¹è¯ä¸­ã€‚"),
        ]
        messages.extend(self.conversation_history)
        messages.append(LLMMessage(role="user", content=task))
        
        # è®°å½• history é•¿åº¦ï¼Œç”¨äºåé¢æå–æ–°æ¶ˆæ¯
        history_len = len(self.conversation_history)
        
        # Tool calling å¾ªç¯
        tool_definitions = self.skill_set.get_tool_definitions()
        max_tool_rounds = 5
        full_response = ""
        
        for tool_round in range(max_tool_rounds):
            if not tool_definitions:
                break
            
            response = await self.provider.chat_complete(
                messages, self.llm_config, tools=tool_definitions
            )
            
            content = response.get("content", "")
            tool_calls = response.get("tool_calls")
            
            if not tool_calls:
                break
            
            # æœ‰å·¥å…·è°ƒç”¨
            messages.append(LLMMessage(
                role="assistant",
                content=content or "",
                tool_calls=tool_calls
            ))
            
            for tc in tool_calls:
                tool_call_id = tc["id"]
                func_name = tc["function"]["name"]
                
                result = await self.skill_set.execute_skill(skill_name=func_name)
                tool_result_str = result.result if result.success else (result.error or "æ‰§è¡Œå¤±è´¥")
                
                messages.append(LLMMessage(
                    role="tool",
                    content=str(tool_result_str) if tool_result_str else "æ— ç»“æœ",
                    tool_call_id=tool_call_id,
                ))
        
        # æœ€ç»ˆæµå¼å›å¤
        async for chunk in self.provider.chat(messages, self.llm_config):
            full_response += chunk
        
        # ===== æ›´æ–°å¯¹è¯å†å²ï¼ˆå®Œæ•´ä¿å­˜ tool calling é“¾ï¼‰=====
        history_start_idx = 1 + len(self.conversation_history)  # 1 for system prompt
        new_messages = messages[history_start_idx:]
        
        for msg in new_messages:
            if msg.role == "tool" and msg.content and len(msg.content) > 1500:
                msg = LLMMessage(
                    role=msg.role,
                    content=msg.content[:1500] + "\n...(ç»“æœå·²æˆªå–å‰1500å­—ç¬¦)",
                    tool_call_id=msg.tool_call_id,
                )
            self.conversation_history.append(msg)
        
        if full_response.strip():
            self.conversation_history.append(LLMMessage(role="assistant", content=full_response))
        
        # æ™ºèƒ½è£å‰ª
        self._trim_conversation_history(max_rounds=6)
        
        return full_response
    
    def _trim_conversation_history(self, max_rounds: int = 6):
        """åŸºäºå¯¹è¯è½®æ¬¡çš„æ™ºèƒ½è£å‰ª + token é¢„ç®—è£å‰ª"""
        if not self.conversation_history:
            return
        
        round_starts = []
        for i, msg in enumerate(self.conversation_history):
            if msg.role == "user":
                round_starts.append(i)
        
        # åŸºç¡€è£å‰ªï¼šæŒ‰è½®æ¬¡
        if len(round_starts) > max_rounds:
            trim_from = round_starts[-max_rounds]
            self.conversation_history = self.conversation_history[trim_from:]
            round_starts = [i for i, m in enumerate(self.conversation_history) if m.role == "user"]
        
        # Token é¢„ç®—è£å‰ª
        MAX_HISTORY_CHARS = 24000
        total_chars = sum(len(m.content or "") for m in self.conversation_history)
        
        while total_chars > MAX_HISTORY_CHARS and len(round_starts) > 2:
            next_round_start = round_starts[1] if len(round_starts) > 1 else len(self.conversation_history)
            removed_chars = sum(len(m.content or "") for m in self.conversation_history[:next_round_start])
            self.conversation_history = self.conversation_history[next_round_start:]
            total_chars -= removed_chars
            round_starts = [i for i, m in enumerate(self.conversation_history) if m.role == "user"]
    
    def extract_session_summary(self) -> Dict[str, Any]:
        """æå–ä¼šè¯æ‘˜è¦"""
        final_report = ""
        if self.conversation_history:
            assistant_msgs = [
                m.content for m in self.conversation_history
                if m.role == "assistant" and m.content and not m.tool_calls
            ]
            if assistant_msgs:
                final_report = assistant_msgs[-1][:2000]
        
        return {"final_report": final_report}


# ============================================================
# æµ‹è¯•ç”¨ä¾‹
# ============================================================

class TestResult:
    def __init__(self, name: str):
        self.name = name
        self.passed = True
        self.errors: List[str] = []
        self.details: List[str] = []
    
    def add_error(self, msg: str):
        self.passed = False
        self.errors.append(msg)
    
    def add_detail(self, msg: str):
        self.details.append(msg)
    
    def __str__(self):
        status = "âœ… PASS" if self.passed else "âŒ FAIL"
        lines = [f"{status} {self.name}"]
        for d in self.details:
            lines.append(f"  ğŸ“ {d}")
        for e in self.errors:
            lines.append(f"  â— {e}")
        return "\n".join(lines)


async def test_01_basic_follow_up_with_tool_results():
    """æµ‹è¯•1ï¼šåŸºç¡€è¿½é—® - æœç´¢æ¨èåè¿½é—®å…·ä½“å†…å®¹ï¼ˆå¤ç°åŸå§‹ bugï¼‰
    
    æ ¸å¿ƒéªŒè¯ï¼š
    - è½®æ¬¡2è¿½é—®"æ‚¬æ¡ˆè§£ç "æ—¶ï¼ŒLLM context ä¸­æ˜¯å¦åŒ…å«ç¬¬1è½®çš„æœç´¢åŸå§‹æ•°æ®
    - è½®æ¬¡3è¿½é—®"çœŸæ¢"æ—¶ï¼ŒLLM context ä¸­æ˜¯å¦åŒ…å«"True Detective"ã€"è±†ç“£9.2"ç­‰ç¬¬1è½®æ•°æ®
    - è½®æ¬¡4ç»¼åˆè¿½é—®æ—¶ï¼ŒLLM context ä¸­æ˜¯å¦åŒæ—¶åŒ…å«å¤šéƒ¨å‰§çš„æ•°æ®
    """
    result = TestResult("åŸºç¡€è¿½é—® - æœç´¢æ¨èåè¿½é—®ï¼ˆå«ä¸Šä¸‹æ–‡è´¨é‡æ–­è¨€ï¼‰")
    
    provider = MockLLMProvider()
    skill_set = MockSkillSet()
    agent = DirectAgentSimulator(provider, skill_set)
    
    # --- è½®æ¬¡ 1ï¼šæœç´¢æ¨è ---
    search_result_data = """æœç´¢ç»“æœï¼š
1. æ‚¬æ¡ˆè§£ç  (Unresolved) - 2024å¹´Netflixæ‚¬ç–‘å‰§ï¼Œè±†ç“£8.9åˆ†ï¼Œè®²è¿°FBIæ¢å‘˜è°ƒæŸ¥è¿ç¯æ‚¬æ¡ˆ
2. çœŸæ¢ (True Detective) - HBOç»å…¸æ‚¬ç–‘å‰§ï¼Œç¬¬ä¸€å­£è±†ç“£9.2åˆ†ï¼Œé©¬ä¿®Â·éº¦åº·çº³ä¸»æ¼”
3. æš—é»‘ (Dark) - å¾·å›½æ‚¬ç–‘ç§‘å¹»å‰§ï¼Œè±†ç“£9.0åˆ†ï¼Œæ—¶é—´æ—…è¡Œ+æ‚¬ç–‘
4. åˆ©å™¨ (Sharp Objects) - HBOè¿·ä½ å‰§ï¼Œè‰¾ç±³Â·äºšå½“æ–¯ä¸»æ¼”ï¼Œå¿ƒç†æ‚¬ç–‘"""
    skill_set.set_result("web-search", search_result_data, "æœç´¢åˆ°4éƒ¨æµ·å¤–æ‚¬ç–‘å‰§")
    
    tc_id = "call_search_001"
    provider.add_response(
        content="è®©æˆ‘æœç´¢ä¸€ä¸‹çƒ­é—¨æµ·å¤–æ‚¬ç–‘å‰§ã€‚",
        tool_calls=[build_tool_call("web-search", {"task": "å¥½çœ‹çš„æµ·å¤–æ‚¬ç–‘å‰§æ¨è"}, tc_id)]
    )
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="æ ¹æ®æœç´¢ç»“æœï¼Œä¸ºä½ æ¨èä»¥ä¸‹æµ·å¤–æ‚¬ç–‘å‰§ï¼š\n1. **æ‚¬æ¡ˆè§£ç ** - Netflix 2024å¹´æ–°ä½œï¼Œè±†ç“£8.9\n2. **çœŸæ¢** - HBOç»å…¸ï¼Œè±†ç“£9.2\n3. **æš—é»‘** - å¾·å›½ç§‘å¹»æ‚¬ç–‘ï¼Œè±†ç“£9.0\n4. **åˆ©å™¨** - HBOè¿·ä½ å‰§ï¼Œå¿ƒç†æ‚¬ç–‘")
    
    resp1 = await agent.execute_task("æ¨èå¥½çœ‹çš„æµ·å¤–æ‚¬ç–‘å‰§")
    
    # éªŒè¯å›å¤è´¨é‡
    for e in assert_response_quality(resp1, ["æ‚¬æ¡ˆè§£ç ", "çœŸæ¢", "æš—é»‘", "åˆ©å™¨"], "è½®æ¬¡1å›å¤"):
        result.add_error(e)
    
    # éªŒè¯ history æ•°æ®å®Œæ•´æ€§
    if not any(m.role == "tool" for m in agent.conversation_history):
        result.add_error("è½®æ¬¡1å conversation_history ä¸­ç¼ºå°‘ tool æ¶ˆæ¯")
    else:
        result.add_detail("âœ“ æœç´¢ç»“æœæ•°æ®å·²ä¿å­˜åˆ° history")
    
    # --- è½®æ¬¡ 2ï¼šè¿½é—®"æ‚¬æ¡ˆè§£ç " ---
    provider.reset()
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="**æ‚¬æ¡ˆè§£ç  (Unresolved)** æ˜¯2024å¹´Netflixæ¨å‡ºçš„æ‚¬ç–‘å‰§ï¼Œè±†ç“£è¯„åˆ†8.9åˆ†ï¼Œè®²è¿°FBIæ¢å‘˜æ·±å…¥è°ƒæŸ¥ä¸€ç³»åˆ—è¿ç¯æ‚¬æ¡ˆ...")
    
    resp2 = await agent.execute_task("ä½ æ¨èçš„æ‚¬æ¡ˆè§£ç èƒ½ä¸èƒ½å±•å¼€è®²è®²")
    
    # â˜… æ ¸å¿ƒæ–­è¨€ï¼šè½®æ¬¡2æ—¶ LLM æ”¶åˆ°çš„ context ä¸­å¿…é¡»åŒ…å«ç¬¬1è½®æœç´¢çš„åŸå§‹æ•°æ®
    for e in assert_context_contains(provider, 
        ["æ‚¬æ¡ˆè§£ç ", "FBIæ¢å‘˜", "è±†ç“£8.9"], description="è½®æ¬¡2ä¸Šä¸‹æ–‡åº”å«æœç´¢æ•°æ®"):
        result.add_error(e)
    for e in assert_context_has_role(provider, "tool", description="è½®æ¬¡2ä¸Šä¸‹æ–‡åº”å«toolæ¶ˆæ¯"):
        result.add_error(e)
    # å›å¤åº”å¼•ç”¨æœç´¢ä¸­çš„å…·ä½“æ•°æ®
    for e in assert_response_quality(resp2, ["æ‚¬æ¡ˆè§£ç ", "Netflix", "2024"], "è½®æ¬¡2å›å¤"):
        result.add_error(e)
    
    result.add_detail("âœ“ è½®æ¬¡2: LLM context åŒ…å«ç¬¬1è½®æœç´¢æ•°æ®ï¼Œå›å¤å¼•ç”¨äº†æ­£ç¡®å†…å®¹")
    
    # --- è½®æ¬¡ 3ï¼šè¿½é—®"çœŸæ¢" ---
    provider.reset()
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="**çœŸæ¢ (True Detective)** ç¬¬ä¸€å­£æ˜¯HBOç»å…¸æ‚¬ç–‘å‰§ï¼Œè±†ç“£9.2åˆ†ï¼Œé©¬ä¿®Â·éº¦åº·çº³é¥°æ¼”çš„æ¢å‘˜æ·±å…¥è°ƒæŸ¥...")
    
    resp3 = await agent.execute_task("çœŸæ¢å‘¢ï¼Ÿ")
    
    # â˜… æ ¸å¿ƒæ–­è¨€ï¼šè½®æ¬¡3æ—¶ LLM context ä»åŒ…å«ç¬¬1è½®çš„æœç´¢æ•°æ®ï¼ˆçœŸæ¢ç›¸å…³ï¼‰
    for e in assert_context_contains(provider,
        ["True Detective", "è±†ç“£9.2", "é©¬ä¿®Â·éº¦åº·çº³"], description="è½®æ¬¡3ä¸Šä¸‹æ–‡åº”å«çœŸæ¢æ•°æ®"):
        result.add_error(e)
    for e in assert_response_quality(resp3, ["çœŸæ¢", "HBO", "9.2"], "è½®æ¬¡3å›å¤"):
        result.add_error(e)
    
    result.add_detail("âœ“ è½®æ¬¡3: LLM context åŒ…å«çœŸæ¢çš„åŸå§‹æœç´¢æ•°æ®")
    
    # --- è½®æ¬¡ 4ï¼šç»¼åˆè¿½é—® ---
    provider.reset()
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="å¦‚æœä½ æ˜¯æ‚¬ç–‘å‰§å…¥é—¨ï¼Œæˆ‘æ¨èä»**çœŸæ¢ç¬¬ä¸€å­£**å¼€å§‹ï¼Œå®ƒè±†ç“£9.2åˆ†æ˜¯æœ€é«˜çš„...")
    
    resp4 = await agent.execute_task("è¿™å‡ éƒ¨å“ªéƒ¨æœ€é€‚åˆå…¥é—¨ï¼Ÿ")
    
    # â˜… æ ¸å¿ƒæ–­è¨€ï¼šè½®æ¬¡4æ—¶ LLM context åº”åŒæ—¶åŒ…å«å¤šéƒ¨å‰§çš„æ•°æ®ï¼ˆæ‰èƒ½åšç»¼åˆæ¨èï¼‰
    for e in assert_context_contains(provider,
        ["æ‚¬æ¡ˆè§£ç ", "çœŸæ¢", "æš—é»‘", "åˆ©å™¨"], description="è½®æ¬¡4ä¸Šä¸‹æ–‡åº”å«æ‰€æœ‰å‰§ç›®"):
        result.add_error(e)
    for e in assert_response_quality(resp4, ["çœŸæ¢", "å…¥é—¨"], "è½®æ¬¡4å›å¤"):
        result.add_error(e)
    
    result.add_detail("âœ“ è½®æ¬¡4: LLM context åŒ…å«æ‰€æœ‰å†å²æ•°æ®ï¼Œå¯åšç»¼åˆåˆ¤æ–­")
    
    # ç»“æ„éªŒè¯
    total_rounds = count_rounds(agent.conversation_history)
    if total_rounds != 4:
        result.add_error(f"åº”æœ‰4è½®å¯¹è¯ï¼Œå®é™… {total_rounds} è½®")
    
    seq_errors = validate_message_sequence(agent.conversation_history)
    for e in seq_errors:
        result.add_error(f"æ¶ˆæ¯åºåˆ—é”™è¯¯: {e}")
    
    return result


async def test_02_multi_tool_calls_in_one_round():
    """æµ‹è¯•2ï¼šå•è½®å¤šå·¥å…·è°ƒç”¨ - LLM åœ¨ä¸€è½®ä¸­è°ƒç”¨å¤šä¸ªå·¥å…·
    
    è½®æ¬¡ï¼š
    1. ç”¨æˆ·ï¼šå¯¹æ¯”åŒ—äº¬å’Œä¸Šæµ·ä»Šå¤©çš„å¤©æ°”
       LLMï¼šè°ƒç”¨æœç´¢(åŒ—äº¬å¤©æ°”) + æœç´¢(ä¸Šæµ·å¤©æ°”) â†’ ç»¼åˆå›å¤
    2. ç”¨æˆ·ï¼šå“ªä¸ªåŸå¸‚æ›´é€‚åˆæˆ·å¤–æ´»åŠ¨ï¼Ÿ
    3. ç”¨æˆ·ï¼šæ˜å¤©å‘¢ï¼Ÿ
    4. ç”¨æˆ·ï¼šæ€»ç»“ä¸€ä¸‹
    """
    result = TestResult("å•è½®å¤šå·¥å…·è°ƒç”¨")
    
    provider = MockLLMProvider()
    skill_set = MockSkillSet()
    agent = DirectAgentSimulator(provider, skill_set)
    
    # --- è½®æ¬¡ 1ï¼šä¸¤ä¸ª tool calls ---
    skill_set.set_result("web-search", "åŒ—äº¬ä»Šå¤©ï¼šæ™´ï¼Œ25Â°Cï¼Œé€‚åˆæˆ·å¤–")
    
    tc1 = build_tool_call("web-search", {"task": "åŒ—äº¬ä»Šå¤©å¤©æ°”"}, "call_bj")
    tc2 = build_tool_call("web-search", {"task": "ä¸Šæµ·ä»Šå¤©å¤©æ°”"}, "call_sh")
    
    provider.add_response(content="æˆ‘æ¥æŸ¥ä¸€ä¸‹ä¸¤ä¸ªåŸå¸‚çš„å¤©æ°”ã€‚", tool_calls=[tc1, tc2])
    # æ³¨æ„ï¼šMockSkillSet å¯¹åŒä¸€ skill åªæœ‰ä¸€ä¸ªç»“æœï¼Œè¿™é‡Œç®€åŒ–
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="åŒ—äº¬ï¼šæ™´25Â°Cï¼Œä¸Šæµ·ï¼šé˜´22Â°Cã€‚åŒ—äº¬æ›´é€‚åˆæˆ·å¤–ã€‚")
    
    await agent.execute_task("å¯¹æ¯”åŒ—äº¬å’Œä¸Šæµ·ä»Šå¤©çš„å¤©æ°”")
    
    # éªŒè¯å¤šä¸ª tool æ¶ˆæ¯éƒ½è¢«ä¿å­˜
    tool_msgs = [m for m in agent.conversation_history if m.role == "tool"]
    result.add_detail(f"è½®æ¬¡1å tool æ¶ˆæ¯æ•°: {len(tool_msgs)}")
    if len(tool_msgs) < 2:
        result.add_error(f"åº”æœ‰2æ¡ tool æ¶ˆæ¯ï¼ˆåŒå·¥å…·è°ƒç”¨ï¼‰ï¼Œå®é™… {len(tool_msgs)} æ¡")
    
    # éªŒè¯ assistant(tool_calls) æ¶ˆæ¯ä¿å­˜äº† tool_calls å­—æ®µ
    tc_msgs = [m for m in agent.conversation_history if m.role == "assistant" and m.tool_calls]
    if not tc_msgs:
        result.add_error("assistant(tool_calls) æ¶ˆæ¯æœªä¿å­˜ tool_calls å­—æ®µ")
    else:
        result.add_detail(f"âœ“ assistant(tool_calls) æ¶ˆæ¯å·²ä¿å­˜ï¼Œå« {len(tc_msgs[0].tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
    
    # --- è½®æ¬¡ 2-4 ---
    for q in ["å“ªä¸ªåŸå¸‚æ›´é€‚åˆæˆ·å¤–æ´»åŠ¨ï¼Ÿ", "æ˜å¤©å‘¢ï¼Ÿ", "æ€»ç»“ä¸€ä¸‹ä¸¤å¤©çš„å¤©æ°”å¯¹æ¯”"]:
        provider.reset()
        provider.add_response(content="", tool_calls=None)
        provider.add_response(content=f"å…³äº{q[:10]}çš„å›å¤...")
        await agent.execute_task(q)
    
    total_rounds = count_rounds(agent.conversation_history)
    if total_rounds != 4:
        result.add_error(f"åº”æœ‰4è½®ï¼Œå®é™… {total_rounds} è½®")
    
    seq_errors = validate_message_sequence(agent.conversation_history)
    for e in seq_errors:
        result.add_error(f"æ¶ˆæ¯åºåˆ—é”™è¯¯: {e}")
    
    return result


async def test_03_no_tool_pure_conversation():
    """æµ‹è¯•3ï¼šçº¯æ–‡æœ¬å¯¹è¯ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰- éªŒè¯å¤šè½®çº¯æ–‡æœ¬ä¸Šä¸‹æ–‡ä¼ é€’
    
    æ ¸å¿ƒéªŒè¯ï¼š
    - æ¯è½® LLM è°ƒç”¨æ—¶æ˜¯å¦èƒ½çœ‹åˆ°ä¹‹å‰æ‰€æœ‰è½®çš„é—®ç­”
    - è½®æ¬¡5ï¼ˆ"å‰æ™¯å¦‚ä½•"ï¼‰æ—¶ context æ˜¯å¦åŒ…å«å‰4è½®è®¨è®ºçš„é‡å­è®¡ç®—æ¦‚å¿µ
    """
    result = TestResult("çº¯æ–‡æœ¬å¯¹è¯ï¼ˆå«ä¸Šä¸‹æ–‡ç´¯ç§¯æ–­è¨€ï¼‰")
    
    provider = MockLLMProvider()
    skill_set = MockSkillSet()
    skill_set.get_tool_definitions = lambda: []
    agent = DirectAgentSimulator(provider, skill_set)
    
    qa_pairs = [
        ("ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ", "é‡å­è®¡ç®—æ˜¯åˆ©ç”¨é‡å­åŠ›å­¦åŸç†ï¼ˆå åŠ æ€ã€çº ç¼ ï¼‰è¿›è¡Œè®¡ç®—çš„æŠ€æœ¯ã€‚"),
        ("å®ƒå’Œç»å…¸è®¡ç®—æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ", "ç»å…¸è®¡ç®—ä½¿ç”¨0å’Œ1çš„æ¯”ç‰¹ï¼Œé‡å­è®¡ç®—ä½¿ç”¨é‡å­æ¯”ç‰¹ï¼ˆqubitï¼‰ï¼Œå¯åŒæ—¶è¡¨ç¤º0å’Œ1ã€‚"),
        ("é‡å­æ¯”ç‰¹æ˜¯ä»€ä¹ˆï¼Ÿ", "é‡å­æ¯”ç‰¹ï¼ˆqubitï¼‰æ˜¯é‡å­è®¡ç®—çš„åŸºæœ¬å•å…ƒï¼Œå…·æœ‰å åŠ æ€ç‰¹æ€§ã€‚"),
        ("ç”¨ä¸€ä¸ªæ¯”å–»æ¥è§£é‡Š", "æƒ³è±¡ä¸€ä¸ªç¡¬å¸ï¼šç»å…¸æ¯”ç‰¹æ˜¯æ­£é¢æˆ–åé¢ï¼Œé‡å­æ¯”ç‰¹æ˜¯ç¡¬å¸åœ¨ç©ºä¸­æ—‹è½¬æ—¶åŒæ—¶æ˜¯ä¸¤é¢ã€‚"),
        ("è¿™ä¸ªé¢†åŸŸçš„å‰æ™¯å¦‚ä½•ï¼Ÿ", "é‡å­è®¡ç®—å‰æ™¯å¹¿é˜”ï¼ŒIBMå’ŒGoogleç­‰éƒ½åœ¨æ¨è¿›ï¼Œä½†ä»é¢ä¸´é€€ç›¸å¹²ç­‰æŠ€æœ¯æŒ‘æˆ˜ã€‚"),
    ]
    
    for i, (q, a) in enumerate(qa_pairs):
        provider.reset()
        provider.add_response(content=a)
        await agent.execute_task(q)
    
    # â˜… æ ¸å¿ƒæ–­è¨€ï¼šæœ€åä¸€è½® LLM æ”¶åˆ°çš„ context åº”åŒ…å«å‰é¢è®¨è®ºçš„å…³é”®æ¦‚å¿µ
    for e in assert_context_contains(provider,
        ["é‡å­åŠ›å­¦", "å åŠ æ€", "qubit", "ç¡¬å¸"],
        description="è½®æ¬¡5ä¸Šä¸‹æ–‡åº”å«å‰4è½®å…³é”®æ¦‚å¿µ"):
        result.add_error(e)
    result.add_detail("âœ“ æœ€åä¸€è½® LLM ä¸Šä¸‹æ–‡åŒ…å«å‰4è½®å…³é”®æ¦‚å¿µ")
    
    total_rounds = count_rounds(agent.conversation_history)
    roles = count_roles(agent.conversation_history)
    
    if total_rounds != 5:
        result.add_error(f"åº”æœ‰5è½®ï¼Œå®é™… {total_rounds} è½®")
    if "tool" in roles:
        result.add_error("çº¯æ–‡æœ¬å¯¹è¯ä¸åº”æœ‰ tool æ¶ˆæ¯")
    if roles.get("user", 0) != roles.get("assistant", 0):
        result.add_error(f"user({roles.get('user')}) å’Œ assistant({roles.get('assistant')}) æ¶ˆæ¯æ•°ä¸åŒ¹é…")
    
    return result


async def test_04_pronoun_reference_across_rounds():
    """æµ‹è¯•4ï¼šè·¨è½®ä»£è¯å¼•ç”¨ - éªŒè¯ LLM ä¸Šä¸‹æ–‡ä¸­åŒ…å«æ­£ç¡®çš„å†å²æ•°æ®æ”¯æ’‘ä»£è¯è§£æ
    
    æ ¸å¿ƒéªŒè¯ï¼š
    - è½®æ¬¡2è¿½é—®"å®ƒçš„å¹¶å‘æ¨¡å‹"æ—¶ï¼ŒLLM context æ˜¯å¦åŒ…å« Go/goroutine çš„æœç´¢æ•°æ®
    - è½®æ¬¡3è¿½é—®"goroutine"æ—¶ï¼ŒLLM context æ˜¯å¦ä»åŒ…å«å®Œæ•´æœç´¢æ•°æ®+å‰è½®å›å¤
    - è½®æ¬¡4è¿½é—®å¯¹æ¯”æ—¶ï¼ŒLLM context æ˜¯å¦åŒæ—¶åŒ…å« Go å’Œ Python çš„æ•°æ®
    """
    result = TestResult("è·¨è½®ä»£è¯å¼•ç”¨ï¼ˆå«ä¸Šä¸‹æ–‡è´¨é‡æ–­è¨€ï¼‰")
    
    provider = MockLLMProvider()
    skill_set = MockSkillSet()
    agent = DirectAgentSimulator(provider, skill_set)
    
    search_data = "Goé€‚åˆé«˜å¹¶å‘åç«¯ï¼Œgoroutineè½»é‡çº§åç¨‹ï¼Œç”±Go runtimeç®¡ç†ï¼›Pythoné€‚åˆAI/MLå’Œå¿«é€Ÿå¼€å‘ï¼Œasyncioæä¾›å¼‚æ­¥IOï¼ŒGILé™åˆ¶å¤šçº¿ç¨‹"
    skill_set.set_result("web-search", search_data)
    
    # è½®æ¬¡1ï¼šæœç´¢
    tc_id = "call_search_compare"
    provider.add_response(
        content="æœç´¢ä¸­...",
        tool_calls=[build_tool_call("web-search", {"task": "Python vs Go åç«¯å¼€å‘"}, tc_id)]
    )
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="Goæ›´é€‚åˆé«˜å¹¶å‘åœºæ™¯ï¼Œæ‹¥æœ‰goroutineã€‚Pythonæ›´é€‚åˆAI/MLã€‚")
    await agent.execute_task("Python å’Œ Go å“ªä¸ªæ›´é€‚åˆå†™åç«¯ï¼Ÿ")
    
    # è½®æ¬¡2ï¼šä»£è¯"å®ƒ"æŒ‡ä»£è¿½é—®
    provider.reset()
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="Go çš„å¹¶å‘æ¨¡å‹åŸºäº CSPï¼ˆé€šä¿¡é¡ºåºè¿›ç¨‹ï¼‰ï¼Œgoroutine æ˜¯å…¶æ ¸å¿ƒ...")
    resp2 = await agent.execute_task("å®ƒçš„å¹¶å‘æ¨¡å‹æ˜¯æ€æ ·çš„ï¼Ÿ")
    
    # â˜… æ ¸å¿ƒæ–­è¨€ï¼šè½®æ¬¡2 LLM context åº”åŒ…å«æœç´¢æ•°æ®ï¼ˆæ‰èƒ½è§£æ"å®ƒ"æŒ‡ Goï¼‰
    for e in assert_context_contains(provider,
        ["goroutine", "Goé€‚åˆé«˜å¹¶å‘"], description="è½®æ¬¡2ä¸Šä¸‹æ–‡åº”å«Goæœç´¢æ•°æ®"):
        result.add_error(e)
    for e in assert_context_has_role(provider, "tool", description="è½®æ¬¡2åº”çœ‹åˆ°å†å²tool"):
        result.add_error(e)
    result.add_detail("âœ“ è½®æ¬¡2: ä¸Šä¸‹æ–‡åŒ…å« goroutine/Go æ•°æ®ï¼Œæ”¯æŒä»£è¯è§£æ")
    
    # è½®æ¬¡3ï¼šå¼•ç”¨å‰è½®å›å¤ä¸­çš„å…·ä½“æœ¯è¯­
    provider.reset()
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="goroutine æ˜¯ Go è¯­è¨€çš„è½»é‡çº§çº¿ç¨‹ï¼Œç”± Go runtime è€Œé OS è°ƒåº¦...")
    resp3 = await agent.execute_task("ä½ åˆšæ‰æåˆ°çš„ goroutine æ˜¯ä»€ä¹ˆï¼Ÿ")
    
    # â˜… æ ¸å¿ƒæ–­è¨€ï¼šè½®æ¬¡3ä¸Šä¸‹æ–‡åº”åŒæ—¶åŒ…å«æœç´¢æ•°æ®å’Œå‰è½®å›å¤ä¸­çš„"CSP"
    for e in assert_context_contains(provider,
        ["goroutine", "CSP"], description="è½®æ¬¡3ä¸Šä¸‹æ–‡åº”å«æœç´¢æ•°æ®+å‰è½®å›å¤"):
        result.add_error(e)
    result.add_detail("âœ“ è½®æ¬¡3: ä¸Šä¸‹æ–‡åŒ…å« goroutine+CSPï¼Œå¯è§£æ'ä½ åˆšæ‰æåˆ°çš„'")
    
    # è½®æ¬¡4ï¼šå¯¹æ¯”è¿½é—®
    provider.reset()
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="Go çš„ goroutine ç”± Go runtime è°ƒåº¦ï¼Œå¯è½»æ¾åˆ›å»ºä¸Šç™¾ä¸‡ä¸ªï¼›Python çš„ asyncio æ˜¯å•çº¿ç¨‹äº‹ä»¶å¾ªç¯ï¼Œå— GIL é™åˆ¶...")
    resp4 = await agent.execute_task("å’Œ Python çš„åç¨‹æ¯”å‘¢ï¼Ÿ")
    
    # â˜… æ ¸å¿ƒæ–­è¨€ï¼šè½®æ¬¡4åº”åŒæ—¶åŒ…å« Go å’Œ Python çš„æ•°æ®
    for e in assert_context_contains(provider,
        ["goroutine", "asyncio", "GIL"], description="è½®æ¬¡4ä¸Šä¸‹æ–‡åº”å«Go+Pythonæ•°æ®"):
        result.add_error(e)
    for e in assert_response_quality(resp4, ["goroutine", "asyncio"], "è½®æ¬¡4å›å¤åº”å¯¹æ¯”ä¸¤è€…"):
        result.add_error(e)
    result.add_detail("âœ“ è½®æ¬¡4: ä¸Šä¸‹æ–‡åŒæ—¶å« Go+Python æ•°æ®ï¼Œæ”¯æŒå¯¹æ¯”å›ç­”")
    
    total_rounds = count_rounds(agent.conversation_history)
    if total_rounds != 4:
        result.add_error(f"åº”æœ‰4è½®ï¼Œå®é™… {total_rounds} è½®")
    
    return result


async def test_05_tool_result_truncation():
    """æµ‹è¯•5ï¼šå·¥å…·ç»“æœæˆªæ–­ - éªŒè¯è¶…é•¿å·¥å…·ç»“æœè¢«æ­£ç¡®æˆªæ–­
    
    è½®æ¬¡ï¼š
    1. ç”¨æˆ·ï¼šæœç´¢æœ€æ–°çš„AIè®ºæ–‡ â†’ è¿”å›è¶…é•¿ç»“æœï¼ˆ>1500å­—ç¬¦ï¼‰
    2. ç”¨æˆ·ï¼šç¬¬ä¸€ç¯‡è®ºæ–‡è®²äº†ä»€ä¹ˆï¼Ÿ
    3. ç”¨æˆ·ï¼šå®ƒçš„æ–¹æ³•è®ºæ˜¯ä»€ä¹ˆï¼Ÿ
    4. ç”¨æˆ·ï¼šæ€»ç»“ä¸€ä¸‹
    """
    result = TestResult("å·¥å…·ç»“æœæˆªæ–­")
    
    provider = MockLLMProvider()
    skill_set = MockSkillSet()
    agent = DirectAgentSimulator(provider, skill_set)
    
    # æ„é€ è¶…é•¿æœç´¢ç»“æœï¼ˆ>1500å­—ç¬¦ï¼‰
    long_result = "AIè®ºæ–‡æœç´¢ç»“æœï¼š\n" + "\n".join([
        f"è®ºæ–‡{i}: {'A' * 100} æ‘˜è¦ï¼š{'B' * 100}" for i in range(20)
    ])
    assert len(long_result) > 1500, f"æµ‹è¯•æ•°æ®å¤ªçŸ­: {len(long_result)}"
    skill_set.set_result("web-search", long_result)
    
    # è½®æ¬¡1
    tc_id = "call_search_papers"
    provider.add_response(
        content="æœç´¢ä¸­...",
        tool_calls=[build_tool_call("web-search", {"task": "æœ€æ–°AIè®ºæ–‡"}, tc_id)]
    )
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="æœç´¢åˆ°ä»¥ä¸‹AIè®ºæ–‡ï¼š\n1. è®ºæ–‡0...\n2. è®ºæ–‡1...")
    await agent.execute_task("æœç´¢æœ€æ–°çš„AIè®ºæ–‡")
    
    # éªŒè¯æˆªæ–­
    tool_msgs = [m for m in agent.conversation_history if m.role == "tool"]
    if tool_msgs:
        tool_content_len = len(tool_msgs[0].content)
        result.add_detail(f"å·¥å…·ç»“æœé•¿åº¦: {tool_content_len} (åŸå§‹: {len(long_result)})")
        if tool_content_len > 1600:  # 1500 + æˆªæ–­æç¤º
            result.add_error(f"å·¥å…·ç»“æœæœªè¢«æˆªæ–­: {tool_content_len} > 1600")
        if "ç»“æœå·²æˆªå–" in tool_msgs[0].content:
            result.add_detail("âœ“ æˆªæ–­æç¤ºå·²æ·»åŠ ")
        else:
            result.add_error("æˆªæ–­æç¤ºç¼ºå¤±")
    else:
        result.add_error("tool æ¶ˆæ¯ç¼ºå¤±")
    
    # è½®æ¬¡2-4
    for q in ["ç¬¬ä¸€ç¯‡è®ºæ–‡è®²äº†ä»€ä¹ˆï¼Ÿ", "å®ƒçš„æ–¹æ³•è®ºæ˜¯ä»€ä¹ˆï¼Ÿ", "æ€»ç»“ä¸€ä¸‹"]:
        provider.reset()
        provider.add_response(content="", tool_calls=None)
        provider.add_response(content=f"å…³äº {q[:10]} ...")
        await agent.execute_task(q)
    
    return result


async def test_06_trim_keeps_recent_rounds():
    """æµ‹è¯•6ï¼šè£å‰ªç­–ç•¥ - è¶…è¿‡ max_rounds æ—¶æ­£ç¡®ä¿ç•™æœ€è¿‘è½®æ¬¡
    
    æ‰§è¡Œ 8 è½®å¯¹è¯ï¼ˆå« tool callingï¼‰ï¼ŒéªŒè¯è£å‰ªåä¿ç•™æœ€è¿‘ 6 è½®
    """
    result = TestResult("è£å‰ªç­–ç•¥ - ä¿ç•™æœ€è¿‘ N è½®")
    
    provider = MockLLMProvider()
    skill_set = MockSkillSet()
    skill_set.get_tool_definitions = lambda: []  # æ— å·¥å…·ï¼Œç®€åŒ–
    agent = DirectAgentSimulator(provider, skill_set)
    
    # æ‰§è¡Œ 8 è½®çº¯æ–‡æœ¬å¯¹è¯
    for i in range(8):
        provider.reset()
        provider.add_response(content=f"è¿™æ˜¯ç¬¬ {i+1} è½®çš„å›å¤ã€‚")
        await agent.execute_task(f"ç¬¬ {i+1} ä¸ªé—®é¢˜")
    
    total_rounds = count_rounds(agent.conversation_history)
    result.add_detail(f"8è½®åå®é™…ä¿ç•™è½®æ¬¡: {total_rounds}")
    
    if total_rounds != 6:
        result.add_error(f"åº”ä¿ç•™æœ€è¿‘6è½®ï¼Œå®é™… {total_rounds} è½®")
    
    # éªŒè¯ä¿ç•™çš„æ˜¯æœ€è¿‘6è½®ï¼ˆç¬¬3-8è½®ï¼‰
    first_user_msg = next(m for m in agent.conversation_history if m.role == "user")
    if "ç¬¬ 3 ä¸ªé—®é¢˜" not in first_user_msg.content:
        result.add_error(f"æœ€æ—©çš„ user æ¶ˆæ¯åº”æ˜¯ç¬¬3è½®ï¼Œå®é™…: {first_user_msg.content}")
    else:
        result.add_detail("âœ“ è£å‰ªæ­£ç¡®ä¿ç•™ç¬¬3-8è½®")
    
    return result


async def test_07_trim_with_tool_calls():
    """æµ‹è¯•7ï¼šå¸¦ tool calling çš„è£å‰ª - éªŒè¯è£å‰ªæ—¶ä¿æŒ tool calling é“¾å®Œæ•´
    
    è½®æ¬¡1-3: å¸¦æœç´¢çš„å¯¹è¯
    è½®æ¬¡4-7: çº¯æ–‡æœ¬å¯¹è¯
    éªŒè¯è£å‰ªå tool calling é“¾çš„å®Œæ•´æ€§
    """
    result = TestResult("å¸¦ tool calling çš„è£å‰ªå®Œæ•´æ€§")
    
    provider = MockLLMProvider()
    skill_set = MockSkillSet()
    agent = DirectAgentSimulator(provider, skill_set)
    
    search_data = "æœç´¢ç»“æœï¼šæµ‹è¯•æ•°æ®"
    skill_set.set_result("web-search", search_data)
    
    # è½®æ¬¡1-3ï¼šå¸¦æœç´¢
    for i in range(3):
        provider.reset()
        tc_id = f"call_{i}"
        provider.add_response(
            content="æœç´¢...",
            tool_calls=[build_tool_call("web-search", {"task": f"query_{i}"}, tc_id)]
        )
        provider.add_response(content="", tool_calls=None)
        provider.add_response(content=f"æœç´¢è½® {i+1} çš„å›å¤")
        await agent.execute_task(f"æœç´¢é—®é¢˜ {i+1}")
    
    # è½®æ¬¡4-7ï¼šçº¯æ–‡æœ¬
    skill_set.get_tool_definitions = lambda: []
    for i in range(4):
        provider.reset()
        provider.add_response(content=f"çº¯æ–‡æœ¬è½® {i+4} çš„å›å¤")
        await agent.execute_task(f"æ–‡æœ¬é—®é¢˜ {i+4}")
    
    total_rounds = count_rounds(agent.conversation_history)
    result.add_detail(f"7è½®åä¿ç•™è½®æ¬¡: {total_rounds}")
    
    # éªŒè¯ä¿ç•™6è½®ï¼ˆè½®æ¬¡2-7ï¼‰
    if total_rounds != 6:
        result.add_error(f"åº”ä¿ç•™6è½®ï¼Œå®é™… {total_rounds} è½®")
    
    # éªŒè¯æ¶ˆæ¯åºåˆ—åˆæ³•æ€§
    seq_errors = validate_message_sequence(agent.conversation_history)
    for e in seq_errors:
        result.add_error(f"æ¶ˆæ¯åºåˆ—é”™è¯¯: {e}")
    
    if not seq_errors:
        result.add_detail("âœ“ è£å‰ªåæ¶ˆæ¯åºåˆ—åˆæ³•")
    
    return result


async def test_08_extract_summary_with_tool_chain():
    """æµ‹è¯•8ï¼šextract_session_summary å…¼å®¹æ€§ - éªŒè¯å¸¦ tool calling æ—¶æ‘˜è¦æå–æ­£ç¡®
    
    è½®æ¬¡ï¼š
    1. æœç´¢ + å›å¤
    2. è¿½é—®
    3. å†è¿½é—®
    4. æ€»ç»“
    éªŒè¯ extract_session_summary è¿”å›æœ€åä¸€æ¡çº¯æ–‡æœ¬ assistant å›å¤
    """
    result = TestResult("extract_session_summary å…¼å®¹æ€§")
    
    provider = MockLLMProvider()
    skill_set = MockSkillSet()
    agent = DirectAgentSimulator(provider, skill_set)
    
    skill_set.set_result("web-search", "æœç´¢æ•°æ®...")
    
    # è½®æ¬¡1ï¼šæœç´¢
    tc_id = "call_s1"
    provider.add_response(
        content="æœç´¢ä¸­...",
        tool_calls=[build_tool_call("web-search", {"task": "test"}, tc_id)]
    )
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="æœç´¢ç»“æœçš„æ€»ç»“å›å¤")
    await agent.execute_task("æœç´¢ä¸€ä¸‹")
    
    # è½®æ¬¡2-4ï¼šçº¯æ–‡æœ¬
    skill_set.get_tool_definitions = lambda: []
    for i, (q, a) in enumerate([
        ("è¿½é—®1", "è¿½é—®1çš„å›å¤"),
        ("è¿½é—®2", "è¿½é—®2çš„å›å¤"),
        ("æ€»ç»“ä¸€ä¸‹", "è¿™æ˜¯æœ€ç»ˆçš„æ€»ç»“å›å¤ï¼ŒåŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯ã€‚"),
    ]):
        provider.reset()
        provider.add_response(content=a)
        await agent.execute_task(q)
    
    summary = agent.extract_session_summary()
    result.add_detail(f"æ‘˜è¦: {summary['final_report'][:80]}...")
    
    # åº”è¿”å›æœ€åä¸€æ¡çº¯æ–‡æœ¬ assistant å›å¤
    if "æœ€ç»ˆçš„æ€»ç»“å›å¤" not in summary["final_report"]:
        result.add_error("æ‘˜è¦åº”ä¸ºæœ€åä¸€æ¡çº¯æ–‡æœ¬å›å¤")
    else:
        result.add_detail("âœ“ æ‘˜è¦æ­£ç¡®æå–æœ€åä¸€æ¡çº¯æ–‡æœ¬å›å¤")
    
    # ä¸åº”è¿”å›ä¸­é—´çš„ assistant(tool_calls) æ¶ˆæ¯
    if "æœç´¢ä¸­" in summary["final_report"]:
        result.add_error("æ‘˜è¦ä¸åº”åŒ…å«ä¸­é—´ tool calling æ¶ˆæ¯")
    
    return result


async def test_09_interleaved_tool_and_text():
    """æµ‹è¯•9ï¼šäº¤æ›¿ä½¿ç”¨å·¥å…·å’Œçº¯æ–‡æœ¬ - éªŒè¯è·¨å·¥å…·/çº¯æ–‡æœ¬è½®æ¬¡çš„ä¸Šä¸‹æ–‡å®Œæ•´æ€§
    
    æ ¸å¿ƒéªŒè¯ï¼š
    - è½®æ¬¡3è¿½é—®"å‰ç¥¥ç‰©"æ—¶ï¼ŒLLM context åŒ…å«è½®æ¬¡2çš„æœç´¢ç»“æœï¼ˆåŒ—äº¬å¥¥è¿2008ï¼‰
    - è½®æ¬¡5è¿½é—®"ç›¸éš”å¤šå°‘å¹´"æ—¶ï¼ŒLLM context åŒæ—¶åŒ…å«ä¸¤æ¬¡æœç´¢ç»“æœï¼ˆ2008+2024ï¼‰
    """
    result = TestResult("äº¤æ›¿ä½¿ç”¨å·¥å…·å’Œçº¯æ–‡æœ¬ï¼ˆå«ä¸Šä¸‹æ–‡è´¨é‡æ–­è¨€ï¼‰")
    
    provider = MockLLMProvider()
    skill_set = MockSkillSet()
    agent = DirectAgentSimulator(provider, skill_set)
    
    search_data_1 = "æœç´¢ç»“æœ1ï¼šåŒ—äº¬å¥¥è¿2008å¹´ä¸¾åŠï¼Œä¸»åœºé¦†é¸Ÿå·¢ï¼Œå‰ç¥¥ç‰©ç¦å¨ƒ"
    search_data_2 = "æœç´¢ç»“æœ2ï¼šå·´é»å¥¥è¿2024å¹´ä¸¾åŠï¼Œä¸»åœºé¦†æ³•å…°è¥¿ä½“è‚²åœº"
    skill_set.set_result("web-search", search_data_1)
    
    # è½®æ¬¡1ï¼šçº¯æ–‡æœ¬
    original_tool_defs = skill_set.get_tool_definitions
    skill_set.get_tool_definitions = lambda: []
    provider.add_response(content="ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„ï¼Ÿ")
    await agent.execute_task("ä½ å¥½")
    skill_set.get_tool_definitions = original_tool_defs
    
    # è½®æ¬¡2ï¼šæœç´¢
    provider.reset()
    tc_id1 = "call_bj_olympic"
    provider.add_response(
        content="æŸ¥è¯¢ä¸­...",
        tool_calls=[build_tool_call("web-search", {"task": "åŒ—äº¬å¥¥è¿ä¼š"}, tc_id1)]
    )
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="åŒ—äº¬å¥¥è¿ä¼šäº2008å¹´ä¸¾åŠã€‚")
    await agent.execute_task("åŒ—äº¬å¥¥è¿ä¼šæ˜¯å“ªå¹´ä¸¾åŠçš„ï¼Ÿ")
    
    # è½®æ¬¡3ï¼šçº¯æ–‡æœ¬è¿½é—®ï¼ˆåŸºäºæœç´¢ç»“æœï¼‰
    provider.reset()
    skill_set.get_tool_definitions = lambda: []
    provider.add_response(content="åŒ—äº¬å¥¥è¿ä¼šçš„å‰ç¥¥ç‰©æ˜¯ç¦å¨ƒï¼Œç”±äº”ä¸ªå½¢è±¡ç»„æˆ...")
    resp3 = await agent.execute_task("å‰ç¥¥ç‰©æ˜¯ä»€ä¹ˆï¼Ÿ")
    skill_set.get_tool_definitions = original_tool_defs
    
    # â˜… æ ¸å¿ƒæ–­è¨€ï¼šè½®æ¬¡3 LLM context åº”åŒ…å«è½®æ¬¡2çš„æœç´¢æ•°æ®
    for e in assert_context_contains(provider,
        ["åŒ—äº¬å¥¥è¿2008", "ç¦å¨ƒ"], description="è½®æ¬¡3ä¸Šä¸‹æ–‡åº”å«åŒ—äº¬å¥¥è¿æœç´¢æ•°æ®"):
        result.add_error(e)
    result.add_detail("âœ“ è½®æ¬¡3: çº¯æ–‡æœ¬è¿½é—®æ—¶ä¸Šä¸‹æ–‡å«è½®æ¬¡2æœç´¢æ•°æ®")
    
    # è½®æ¬¡4ï¼šæœç´¢
    provider.reset()
    skill_set.set_result("web-search", search_data_2)
    tc_id2 = "call_paris_olympic"
    provider.add_response(
        content="æŸ¥è¯¢ä¸­...",
        tool_calls=[build_tool_call("web-search", {"task": "å·´é»å¥¥è¿ä¼š"}, tc_id2)]
    )
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="å·´é»å¥¥è¿ä¼šäº2024å¹´ä¸¾åŠã€‚")
    await agent.execute_task("æœ€è¿‘ä¸€å±Šå¥¥è¿ä¼šå‘¢ï¼Ÿ")
    
    # è½®æ¬¡5ï¼šçº¯æ–‡æœ¬å¯¹æ¯”è¿½é—®
    provider.reset()
    skill_set.get_tool_definitions = lambda: []
    provider.add_response(content="ä¸¤å±Šå¥¥è¿ä¼šç›¸éš”16å¹´ï¼ŒåŒ—äº¬2008åˆ°å·´é»2024ã€‚")
    resp5 = await agent.execute_task("ä¸¤å±Šç›¸éš”å¤šå°‘å¹´ï¼Ÿ")
    
    # â˜… æ ¸å¿ƒæ–­è¨€ï¼šè½®æ¬¡5 LLM context åº”åŒæ—¶åŒ…å«ä¸¤æ¬¡æœç´¢æ•°æ®
    for e in assert_context_contains(provider,
        ["åŒ—äº¬å¥¥è¿2008", "å·´é»å¥¥è¿2024"], description="è½®æ¬¡5ä¸Šä¸‹æ–‡åº”å«ä¸¤æ¬¡æœç´¢æ•°æ®"):
        result.add_error(e)
    for e in assert_response_quality(resp5, ["16å¹´", "2008", "2024"], "è½®æ¬¡5å›å¤åº”å«ä¸¤ä¸ªå¹´ä»½"):
        result.add_error(e)
    result.add_detail("âœ“ è½®æ¬¡5: ä¸Šä¸‹æ–‡åŒæ—¶å«ä¸¤æ¬¡æœç´¢æ•°æ®ï¼Œå¯åšè·¨è½®å¯¹æ¯”")
    
    total_rounds = count_rounds(agent.conversation_history)
    if total_rounds != 5:
        result.add_error(f"åº”æœ‰5è½®ï¼Œå®é™… {total_rounds} è½®")
    
    seq_errors = validate_message_sequence(agent.conversation_history)
    for e in seq_errors:
        result.add_error(f"æ¶ˆæ¯åºåˆ—é”™è¯¯: {e}")
    
    return result


async def test_10_deep_reference_chain():
    """æµ‹è¯•10ï¼šæ·±åº¦å¼•ç”¨é“¾ - ç¬¬4è½®å¼•ç”¨ç¬¬1è½®çš„å…·ä½“æ•°æ®
    
    æ ¸å¿ƒéªŒè¯ï¼š
    - è½®æ¬¡2è¿½é—®"ç¬¬3é¦–æ­Œ"æ—¶ï¼ŒLLM context åŒ…å«æ­Œæ›²åˆ—è¡¨ï¼ˆæ‰èƒ½å®šä½ç¬¬3é¦–ï¼‰
    - è½®æ¬¡3åˆ‡æ¢è¯é¢˜åï¼Œè½®æ¬¡4å›åˆ°éŸ³ä¹è¯é¢˜æ—¶ï¼ŒLLM context ä»åŒ…å«å®Œæ•´æ­Œæ›²åˆ—è¡¨
    - éªŒè¯ LLM ä¼ å…¥çš„ messages ä¸­æœç´¢ç»“æœçš„æ•°æ®ç²’åº¦è¶³ä»¥å›ç­”å…·ä½“è¿½é—®
    """
    result = TestResult("æ·±åº¦å¼•ç”¨é“¾ - è·¨å¤šè½®å›æº¯ï¼ˆå«ä¸Šä¸‹æ–‡è´¨é‡æ–­è¨€ï¼‰")
    
    provider = MockLLMProvider()
    skill_set = MockSkillSet()
    agent = DirectAgentSimulator(provider, skill_set)
    
    song_data = """çƒ­é—¨æ­Œæ›²æœç´¢ç»“æœï¼š
1. "Die With A Smile" - Lady Gaga & Bruno Mars
2. "APT." - ROSÃ‰ & Bruno Mars  
3. "Birds of a Feather" - Billie Eilish
4. "Espresso" - Sabrina Carpenter
5. "Beautiful Things" - Benson Boone"""
    skill_set.set_result("web-search", song_data)
    
    # è½®æ¬¡1ï¼šæœç´¢æ­Œæ›²
    tc_id = "call_songs"
    provider.add_response(
        content="æœç´¢ä¸­...",
        tool_calls=[build_tool_call("web-search", {"task": "2024å¹´æœ€ç«çš„5é¦–æ­Œ"}, tc_id)]
    )
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="2024å¹´æœ€ç«çš„5é¦–æ­Œï¼š1. Die With A Smile 2. APT. 3. Birds of a Feather 4. Espresso 5. Beautiful Things")
    await agent.execute_task("2024å¹´æœ€ç«çš„5é¦–æ­Œæ˜¯ä»€ä¹ˆï¼Ÿ")
    
    # è½®æ¬¡2ï¼šè¿½é—®ç¬¬3é¦–
    provider.reset()
    provider.add_response(content="", tool_calls=None)
    provider.add_response(content="Birds of a Feather æ˜¯ Billie Eilish çš„æ­Œï¼Œæ¥è‡ªä¸“è¾‘ HIT ME HARD AND SOFT...")
    resp2 = await agent.execute_task("ç¬¬3é¦–æ­Œè®²äº†ä»€ä¹ˆï¼Ÿ")
    
    # â˜… æ ¸å¿ƒæ–­è¨€ï¼šè½®æ¬¡2 LLM éœ€è¦çœ‹åˆ°å®Œæ•´æ­Œæ›²åˆ—è¡¨æ‰èƒ½å®šä½"ç¬¬3é¦–"
    for e in assert_context_contains(provider,
        ["Birds of a Feather", "Billie Eilish", "Die With A Smile", "Espresso"],
        description="è½®æ¬¡2ä¸Šä¸‹æ–‡åº”å«å®Œæ•´æ­Œæ›²åˆ—è¡¨"):
        result.add_error(e)
    for e in assert_response_quality(resp2, ["Birds of a Feather", "Billie Eilish"], "è½®æ¬¡2å›å¤"):
        result.add_error(e)
    result.add_detail("âœ“ è½®æ¬¡2: ä¸Šä¸‹æ–‡å«å®Œæ•´æ­Œæ›²åˆ—è¡¨ï¼Œå¯å®šä½'ç¬¬3é¦–'")
    
    # è½®æ¬¡3ï¼šåˆ‡æ¢è¯é¢˜
    provider.reset()
    skill_set.get_tool_definitions = lambda: []
    provider.add_response(content="ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå‡ºé—¨ã€‚")
    await agent.execute_task("ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    
    # è½®æ¬¡4ï¼šå›åˆ°éŸ³ä¹è¯é¢˜ï¼Œå¼•ç”¨ç¬¬1è½®
    provider.reset()
    provider.add_response(content="ä½ ä¹‹å‰é—®çš„æ—¶å€™ï¼Œç¬¬ä¸€é¦–æ˜¯ Die With A Smileï¼ŒLady Gaga å’Œ Bruno Mars åˆä½œçš„ã€‚")
    resp4 = await agent.execute_task("ä½ ä¹‹å‰æ¨èçš„ç¬¬ä¸€é¦–æ­Œæ˜¯ä»€ä¹ˆï¼Ÿ")
    
    # â˜… æ ¸å¿ƒæ–­è¨€ï¼šç»è¿‡è¯é¢˜åˆ‡æ¢åï¼Œè½®æ¬¡4çš„ LLM context ä»åŒ…å«ç¬¬1è½®çš„æ­Œæ›²æ•°æ®
    for e in assert_context_contains(provider,
        ["Die With A Smile", "Lady Gaga", "Bruno Mars"],
        description="è½®æ¬¡4ä¸Šä¸‹æ–‡åº”ä»å«ç¬¬1è½®æœç´¢æ•°æ®"):
        result.add_error(e)
    for e in assert_context_has_role(provider, "tool",
        description="è½®æ¬¡4åº”ä»èƒ½çœ‹åˆ°å†å²toolæ¶ˆæ¯"):
        result.add_error(e)
    for e in assert_response_quality(resp4, ["Die With A Smile", "Lady Gaga"], "è½®æ¬¡4å›å¤"):
        result.add_error(e)
    result.add_detail("âœ“ è½®æ¬¡4: è¯é¢˜åˆ‡æ¢åä»ä¿ç•™ç¬¬1è½®æœç´¢æ•°æ®")
    
    # éªŒè¯æœç´¢æ•°æ®ç¡®å®åœ¨ conversation_history çš„ tool æ¶ˆæ¯ä¸­
    tool_msgs = [m for m in agent.conversation_history if m.role == "tool"]
    has_song_data = any("Die With A Smile" in m.content for m in tool_msgs)
    if not has_song_data:
        result.add_error("è½®æ¬¡1çš„æœç´¢åŸå§‹æ•°æ®åœ¨åç»­è½®æ¬¡ä¸­ä¸¢å¤±äº†")
    
    total_rounds = count_rounds(agent.conversation_history)
    if total_rounds != 4:
        result.add_error(f"åº”æœ‰4è½®ï¼Œå®é™… {total_rounds} è½®")
    
    return result


async def test_11_token_budget_trim():
    """æµ‹è¯•11ï¼šToken é¢„ç®—è£å‰ª - å½“å†å²è¶…é•¿æ—¶è‡ªåŠ¨ç¼©å‡è½®æ¬¡
    
    æ„é€ æ¯è½®äº§ç”Ÿå¤§é‡å­—ç¬¦ï¼ˆ>5000å­—ç¬¦çš„ tool ç»“æœï¼‰ï¼Œ
    éªŒè¯ token é¢„ç®—æœºåˆ¶åœ¨ä¿ç•™è½®æ¬¡çš„åŒæ—¶æ§åˆ¶æ€»é•¿åº¦ã€‚
    """
    result = TestResult("Token é¢„ç®—è£å‰ª")
    
    provider = MockLLMProvider()
    skill_set = MockSkillSet()
    agent = DirectAgentSimulator(provider, skill_set)
    
    # æ¯è½®æœç´¢äº§ç”Ÿ 6000 å­—ç¬¦çš„ç»“æœï¼ˆä¼šè¢«æˆªæ–­åˆ° 1500 + åç»­è¿˜æœ‰ assistant å›å¤çº¦ 200 å­—ç¬¦ï¼‰
    # 6è½®åï¼šæ¯è½®çº¦ 1700 å­—ç¬¦ * 6 = 10200 å­—ç¬¦ â†’ åœ¨é¢„ç®—å†…
    # ä½†å¦‚æœå·¥å…·ç»“æœåªæˆªåˆ° 1500ï¼Œå®é™…æ¯è½®è¿˜æœ‰ user(~30) + assistant_tc(~20) + tool(1500) + assistant(200) = ~1750
    
    # æ„é€ è¶…å¤§æœç´¢ç»“æœ
    huge_result = "æœç´¢ç»“æœï¼š" + "æ•°æ®" * 3000  # çº¦ 6006 å­—ç¬¦
    skill_set.set_result("web-search", huge_result)
    
    for i in range(5):
        provider.reset()
        tc_id = f"call_big_{i}"
        provider.add_response(
            content="æœç´¢...",
            tool_calls=[build_tool_call("web-search", {"task": f"big query {i}"}, tc_id)]
        )
        provider.add_response(content="", tool_calls=None)
        provider.add_response(content=f"è¿™æ˜¯ç¬¬ {i+1} è½®çš„é•¿å›å¤ã€‚" + "å†…å®¹" * 200)  # çº¦ 400 å­—ç¬¦
        await agent.execute_task(f"æœç´¢å¤§é‡æ•°æ® {i+1}")
    
    total_chars = sum(len(m.content or "") for m in agent.conversation_history)
    total_rounds = count_rounds(agent.conversation_history)
    
    result.add_detail(f"5è½®é•¿ç»“æœå: {total_rounds} è½®, {total_chars} å­—ç¬¦, {len(agent.conversation_history)} æ¶ˆæ¯")
    
    # æ€»å­—ç¬¦åº”ä¸è¶…è¿‡ 24000
    if total_chars > 24000:
        result.add_error(f"æ€»å­—ç¬¦æ•° {total_chars} è¶…è¿‡é¢„ç®— 24000")
    else:
        result.add_detail(f"âœ“ æ€»å­—ç¬¦æ•° {total_chars} åœ¨é¢„ç®— 24000 ä»¥å†…")
    
    # è‡³å°‘ä¿ç•™ 2 è½®
    if total_rounds < 2:
        result.add_error(f"è‡³å°‘åº”ä¿ç•™2è½®ï¼Œå®é™… {total_rounds}")
    
    return result


# ============================================================
# æµ‹è¯•è¿è¡Œå™¨
# ============================================================

ALL_TESTS = [
    test_01_basic_follow_up_with_tool_results,
    test_02_multi_tool_calls_in_one_round,
    test_03_no_tool_pure_conversation,
    test_04_pronoun_reference_across_rounds,
    test_05_tool_result_truncation,
    test_06_trim_keeps_recent_rounds,
    test_07_trim_with_tool_calls,
    test_08_extract_summary_with_tool_chain,
    test_09_interleaved_tool_and_text,
    test_10_deep_reference_chain,
    test_11_token_budget_trim,
]


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 70)
    print("å¤šè½®å¯¹è¯æµ‹è¯•é›† - DirectAgent conversation_history ç®¡ç†")
    print("=" * 70)
    print()
    
    results: List[TestResult] = []
    
    for test_func in ALL_TESTS:
        try:
            r = await test_func()
            results.append(r)
        except Exception as e:
            r = TestResult(test_func.__doc__.split("\n")[0] if test_func.__doc__ else test_func.__name__)
            r.add_error(f"æµ‹è¯•å¼‚å¸¸: {type(e).__name__}: {e}")
            import traceback
            r.add_detail(traceback.format_exc())
            results.append(r)
    
    # è¾“å‡ºç»“æœ
    print()
    for r in results:
        print(r)
        print()
    
    # ç»Ÿè®¡
    passed = sum(1 for r in results if r.passed)
    failed = sum(1 for r in results if not r.passed)
    
    print("=" * 70)
    print(f"æµ‹è¯•ç»“æœ: {passed}/{len(results)} é€šè¿‡, {failed} å¤±è´¥")
    print("=" * 70)
    
    # åˆ†æ
    if failed > 0:
        print("\nâŒ å¤±è´¥æµ‹è¯•åˆ†æï¼š")
        for r in results:
            if not r.passed:
                print(f"\n  [{r.name}]")
                for e in r.errors:
                    print(f"    - {e}")
    
    return results


if __name__ == "__main__":
    results = asyncio.run(run_all_tests())
    # é€€å‡ºç ï¼šæœ‰å¤±è´¥åˆ™è¿”å› 1
    sys.exit(0 if all(r.passed for r in results) else 1)
